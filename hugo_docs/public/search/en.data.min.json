[{"id":0,"href":"/CYBERTEC-pg-operator/","title":"CPO (CYBERTEC-PG-Operator)","parent":"","content":"Current Release: 0.7.0 (15.07.2024) Release Notes\nCPO (CYBERTEC PG Operator) allows you to create and run PostgreSQL clusters on Kubernetes.\nThe operator reduces your efforts and simplifies the administration of your PostgreSQL clusters so that you can concentrate on other things.\nThe following features characterise our operator:\nDeclarative mode of operation Takes over all the necessary steps for setting up and managing the PG cluster. Integrated backup solution, automatic backups and very easy restore (snapshot \u0026amp; PITR) Rolling update procedure for adjustments to the pods and minor updates Major upgrade with minimum interruption time Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing CPO is tested on the following platforms:\nKubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 Rancher AWS EKS Azure AKS Google GKE Furthermore, CPO is basically executable on any CSCF-certified Kubernetes platform.\n","description":"Current Release: 0.7.0 (15.07.2024) Release Notes\nCPO (CYBERTEC PG Operator) allows you to create and run PostgreSQL clusters on Kubernetes.\nThe operator reduces your efforts and simplifies the administration of your PostgreSQL clusters so that you can concentrate on other things.\nThe following features characterise our operator:\nDeclarative mode of operation Takes over all the necessary steps for setting up and managing the PG cluster. Integrated backup solution, automatic backups and very easy restore (snapshot \u0026amp; PITR) Rolling update procedure for adjustments to the pods and minor updates Major upgrade with minimum interruption time Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing CPO is tested on the following platforms:"},{"id":1,"href":"/CYBERTEC-pg-operator/project/","title":"CPO","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":2,"href":"/CYBERTEC-pg-operator/project/project/","title":"The Project","parent":"CPO","content":"","description":""},{"id":3,"href":"/CYBERTEC-pg-operator/project/project-copy/","title":"Container Images","parent":"CPO","content":"","description":""},{"id":4,"href":"/CYBERTEC-pg-operator/architecture/","title":"Architecture","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":5,"href":"/CYBERTEC-pg-operator/documentation/","title":"Documentation","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":6,"href":"/CYBERTEC-pg-operator/architecture/architecture/","title":"Architecture","parent":"Architecture","content":"This chapter covers all important aspects relating to the architecture of CPO and the associated components. In addition to the underlying Kubertnetes, the various components and their interaction for the operation of a PostgreSQL cluster are analysed.\nBrief overview of the components Network-Traffic PG-Cluster-intern Traffic With internal PG cluster-internal traffic, we are talking about all traffic that is necessary for the operation of the cluster itself. This includes\nCommunication for the sync of the replicas: pg_basebackup \u0026amp; streaming replication Communication with pgBackRest (if configured) Backups WAL archiving replica-create for new replicas The figure below shows the internal traffic flows with pgBackRest based on block storage (left) or cloud storage (right)\nExternal Traffic External traffic, i.e. the connection to the database for the user or the application, takes place via defined Kubernetes services. A distinction must be made here between read/write and read only traffic.\nread/write read-only ","description":"This chapter covers all important aspects relating to the architecture of CPO and the associated components. In addition to the underlying Kubertnetes, the various components and their interaction for the operation of a PostgreSQL cluster are analysed.\nBrief overview of the components Network-Traffic PG-Cluster-intern Traffic With internal PG cluster-internal traffic, we are talking about all traffic that is necessary for the operation of the cluster itself. This includes\nCommunication for the sync of the replicas: pg_basebackup \u0026amp; streaming replication Communication with pgBackRest (if configured) Backups WAL archiving replica-create for new replicas The figure below shows the internal traffic flows with pgBackRest based on block storage (left) or cloud storage (right)"},{"id":7,"href":"/CYBERTEC-pg-operator/crd/crd-postgresql/","title":"PostgreSQL","parent":"References","content":" postgresql Name Type required Description apiVersion string true acid.zalando.do/v1 kind string true metadata object true spec object true status object false spec Name Type required Description backup object false Enables the definition of a customised backup solution for the cluster teamId string true name of the team the cluster belongs to numberOfInstances Int true Number of nodes of the cluster dockerImages string false Define a custom image to override the default schedulerName string false Define a custom Name to override the default spiloRunAsUser string false Define an User id which should be used for the pods spiloRunAsGroup string false Define an Group id which should be used for the pods spiloFSGroup string false Persistent Volumes for the pods in the StatefulSet will be owned and writable by the group ID specified. enableMasterLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres primary enableMasterPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the primary ConnectionPooler enableReplicaLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres replicas enableReplicaPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the Replica-ConnectionPooler allowedSourceRange string false Defines the range of IP networks (in CIDR-notation). The corresponding load balancer is accessible only to the networks defined by this parameter. users map false a map of usernames to user flags for the users that should be created in the cluster by the operator usersWithSecretRotation list false list of users to enable credential rotation in K8s secrets. The rotation interval can only be configured globally. usersWithInPlaceSecretRotation list false list of users to enable in-place password rotation in K8s secrets. The rotation interval can only be configured globally. databases map false a map of databases that should be created in the cluster by the operator tolerations list false a list of tolerations that apply to the cluster pods. Each element of that list is a dictionary with the following fields: key, operator, value, effect and tolerationSeconds podPriorityClassName string false a name of the priority class that should be assigned to the cluster pods. If not set then the default priority class is taken. The priority class itself must be defined in advance podAnnotations map false A map of key value pairs that gets attached as annotations to each pod created for the database. ServiceAnnotations map false A map of key value pairs that gets attached as annotations to each Service created for the database. enableShmVolume boolean false Start a database pod without limitations on shm memory. By default Docker limit /dev/shm to 64M (see e.g. the docker issue, which could be not enough if PostgreSQL uses parallel workers heavily. If this option is present and value is true, to the target database pod will be mounted a new tmpfs volume to remove this limitation. enableConnectionPooler boolean false creates a ConnectionPooler for the primary Database enableReplicaConnectionPooler boolean false creates a ConnectionPooler for the replica Databases enableLogicalBackup boolean false Enable logical Backups for this Cluster (Stored on S3) - s3-configuration for Operator is needed (Not for pgBackRest) logicalBackupSchedule string false Schedule for the logical backup K8s cron job. (Not for pgBackRest) additionalVolumes list false List of additional volumes to mount in each container of the statefulset pod. Each item must contain a name, mountPath, and volumeSource which is a kubernetes volumeSource. It allows you to mount existing PersistentVolumeClaims, ConfigMaps and Secrets inside the StatefulSet. back to parent\nbackup Name Type required Description pgbackrest object false Enables the definition of a pgbackrest-setup for the cluster back to parent\npgbackrest Name Type required Description configuration object false Enables the definition of a pgbackrest-setup for the cluster global object false image string true repos array true resources: object false Resource definition (limits.cpu, limits.memory \u0026amp; requests.cpu \u0026amp; requests.memory) back to parent\nconfiguration Name Type required Description secret object false Secretname with the contained S3 credentials (AccessKey \u0026amp; SecretAccessKey) (Note: must be placed in the same namespace as the cluster) protection object false Enable Protection-Options back to parent\nprotection Name Type required Description restore boolean false A restore is ignored as long as this option is set to true. back to parent\nrepos Name Type required Description name string true Name of the repository Required:Repo[1-4] storage string true Defines the used backup-storage (Choose from List: pvc,s3,blob,gcs) resource string true Bucket-/Instance-/Storage- or PVC-Name endpoint string false The Endpoint for the choosen Storage (Not required for local storage) region string false Region for the choosen Storage (S3 only) schedule string false Object for defining automatic backups back to parent\nschedule Name Type required Description full string false (Cron-Syntax) Define full backup incr string false (Cron-Syntax) Define incremental backup diff string false (Cron-Syntax) Define differential backup back to parent\nstatus Name Type required Description PostgresClusterStatus string false Shows the cluster status. Filled by the Operator back to parent\n","description":"postgresql Name Type required Description apiVersion string true acid.zalando.do/v1 kind string true metadata object true spec object true status object false spec Name Type required Description backup object false Enables the definition of a customised backup solution for the cluster teamId string true name of the team the cluster belongs to numberOfInstances Int true Number of nodes of the cluster dockerImages string false Define a custom image to override the default schedulerName string false Define a custom Name to override the default spiloRunAsUser string false Define an User id which should be used for the pods spiloRunAsGroup string false Define an Group id which should be used for the pods spiloFSGroup string false Persistent Volumes for the pods in the StatefulSet will be owned and writable by the group ID specified."},{"id":8,"href":"/CYBERTEC-pg-operator/crd/crd-operator-configurator/","title":"Operator-Configuration","parent":"References","content":" Name Type default Description enable_crd_registration boolean true crd_categories string all enable_lazy_spilo_upgrade boolean false enable_pgversion_env_var boolean true enable_spilo_wal_path_combat boolean false etcd_host string kubernetes_use_configmaps boolean false docker_image string sidecars list enable_shm_volume boolean true workers int 8 max_instances int -1 min_instances int -1 resync_period string 30m repair_period string 5m set_memory_request_to_limit boolean false debug_logging boolean true enable_db_access boolean true spilo_privileged boolean false spilo_allow_privilege_escalation boolean true watched_namespace string * major-upgrade-specific Name Type default Description major_version_upgrade_mode string off major_version_upgrade_team_allow_list string minimal_major_version string 9.6 target_major_version string 14 aws-specific Name Type default Description wal_s3_bucket string log_s3_bucket string kube_iam_role string aws_region string additional_secret_mount string additional_secret_mount_path string enable_ebs_gp3_migration boolean enable_ebs_gp3_migration_max_size int logical-backup-specific Name Type default Description logical_backup_docker_image string logical_backup_google_application_credentials string logical_backup_job_prefix string logical_backup_provider string logical_backup_s3_access_key_id string logical_backup_s3_bucket string logical_backup_s3_endpoint string logical_backup_s3_region string logical_backup_s3_secret_access_key string logical_backup_s3_sse string logical_backup_s3_retention_time string logical_backup_schedule string (Cron-Syntax) team-api-specific Name Type default Description enable_teams_api string teams_api_url string teams_api_role_configuration string enable_team_superuser boolean team_admin_role boolean enable_admin_role_for_users boolean pam_role_name string pam_configuration string protected_role_names list postgres_superuser_teams string role_deletion_suffix string enable_team_member_deprecation boolean enable_postgres_team_crd boolean enable_postgres_team_crd_superusers boolean enable_team_id_clustername_prefix boolean ","description":"Name Type default Description enable_crd_registration boolean true crd_categories string all enable_lazy_spilo_upgrade boolean false enable_pgversion_env_var boolean true enable_spilo_wal_path_combat boolean false etcd_host string kubernetes_use_configmaps boolean false docker_image string sidecars list enable_shm_volume boolean true workers int 8 max_instances int -1 min_instances int -1 resync_period string 30m repair_period string 5m set_memory_request_to_limit boolean false debug_logging boolean true enable_db_access boolean true spilo_privileged boolean false spilo_allow_privilege_escalation boolean true watched_namespace string * major-upgrade-specific Name Type default Description major_version_upgrade_mode string off major_version_upgrade_team_allow_list string minimal_major_version string 9."},{"id":9,"href":"/CYBERTEC-pg-operator/quickstart/","title":"Quickstart","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":10,"href":"/CYBERTEC-pg-operator/quickstart/quickstart/","title":"Quickstart","parent":"Quickstart","content":"We can tell and document so much about our project but it seems you just want to get started. Let us show you the fastest way to use CPO.\nPreconditions git helm (optional) kubectl or oc Let\u0026rsquo;s start Step 1 - Preparations To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials Step 2 - Install the Operator Two options are available for the installation:\nInstallation via Helm-Chart Installation via apply Installation via Helm-Chart kubectl apply -k setup/namespace/. helm install cpo setup/helm/operator/ -n cpo Installation via apply kubectl apply -k setup/namespace/. kubectl apply -k setup/helm/operator/. -n cpo You can check if the operator pod is in operation.\nkubectl get pods -n cpo --selector=cpo.cybertec.at/pod/type=postgres-operator The result should look like this:\nNAME READY STATUS RESTARTS AGE postgres-operator-599688d948-fw8pw 1/1 Running 0 41s The operator is ready and the setup is complete. The next step is the creation of a Postgres cluster\nStep 3 - Create a Cluster To create a simple cluster, the following command is sufficient\nkubectl apply -k cluster-tutorials/single-cluster watch kubectl get pods --selector cluster-name=cluster-1,application=spilo The result should look like this:\nAlle 2.0s: kubectl get pods --selector cluster-name=cluster-1,application=spilo NAME READY STATUS RESTARTS AGE cluster-1-0 2/2 Running 0 28s cluster-1-1 0/2 PodInitializing 0 9s Step 4 - Connect to the Database Get your login information from the secret.\nkubectl get secret postgres.acid-cluster-1.credentials.postgresql.acid.zalan.do -o jsonpath=\u0026#39;{.data}\u0026#39; | jq \u0026#39;.|map_values(@base64d)\u0026#39; The result should look like this:\n{ \u0026#34;password\u0026#34;: \u0026#34;2rZG1Kx9asdHscswQGzff4Ru0xW6uasacy3GQ0sjdCH3wWr0kguUXUZek6dkemsf\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;postgres\u0026#34; } Connection via port-forward port-forward acid-cluster-1-1 5432:5432 # using psql PGPASSWORD=2rZG1Kx9asdHscswQGzffjdCH3wWr0kguUXUZek6dkemsf psql -h 127.0.0.1 -p 5432 -U postgres # using usql PGPASSWORD=2rZG1Kx9asdHscswQGzffjdCH3wWr0kguUXUZek6dkemsf usql postgresql://postgres@127.0.0.1/postgres Next Steps Congratulations, your first cluster is ready and you were able to connect to it. On the following pages we have put together an introduction with lots of information and details to show you the different possibilities and components of CPO.\n","description":"We can tell and document so much about our project but it seems you just want to get started. Let us show you the fastest way to use CPO.\nPreconditions git helm (optional) kubectl or oc Let\u0026rsquo;s start Step 1 - Preparations To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials Step 2 - Install the Operator Two options are available for the installation:"},{"id":11,"href":"/CYBERTEC-pg-operator/installation/","title":"Installation","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":12,"href":"/CYBERTEC-pg-operator/installation/playground.md/","title":"Setup local Kubernetes","parent":"Installation","content":"","description":""},{"id":13,"href":"/CYBERTEC-pg-operator/installation/installation/","title":"Install CPO","parent":"Installation","content":" Prerequisites For the installation you either need our CPO tutorial repository or you install CPO directly from our registry. Exception: Installation via Operatorhub (Openshift only)\nCPO-Tutorial-Repository To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials CPO-Registry Create Namespace # kubectl kubectl create namespace cpo # oc oc create namespace cpo Install CPO There are several ways to install CPO:\nUse Helm Use apply Use Operatorhub (On Openshift only) Helm You can check and change the value.yaml of the helm diagram under the path helm/operator/values.yaml By default, the operator is defined so that it is configured via crd-configuration. If you wish, you can change this to configmap. There are also some other default settings.\nhelm install -n cpo cpo helm/operator/. The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\nApply The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\nOperatorhub The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\n","description":"Prerequisites For the installation you either need our CPO tutorial repository or you install CPO directly from our registry. Exception: Installation via Operatorhub (Openshift only)\nCPO-Tutorial-Repository To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials CPO-Registry Create Namespace # kubectl kubectl create namespace cpo # oc oc create namespace cpo Install CPO There are several ways to install CPO:"},{"id":14,"href":"/CYBERTEC-pg-operator/installation/configuration_operator/","title":"Operator-Configuration","parent":"Installation","content":"Users who are already used to working with PostgreSQL from Baremetal or VMs are already familiar with the need for various files to configure PostgreSQL. These include\npostgresql.conf pg_hba.conf \u0026hellip; Although these files are available in the container, direct modification is not planned. As part of the declarative mode of operation of the operator, these files are defined via the operator. The modifying intervention within the container also represents a contradiction to the immutability of the container.\nFor these reasons, the operator provides a way to make adjustments to the various files, from PostgreSQL to Patroni.\nWe differentiate between two main objects in the cluster manifest:\npostgresql with the child objects version and parameters patroni with objects for the pg_hab, slots and much more postgresql The postgresql object consists of the following elements:\nversion - allows you to select the major version of PostgreSQL used. parameters- enables the postgresql.conf to be changed spec: postgresql: parameters: shared_preload_libraries: \u0026#39;pg_stat_statements,pgnodemx, timescaledb\u0026#39; shared_buffers: \u0026#39;512MB\u0026#39; version: \u0026#39;16\u0026#39; Any known PostgreSQL parameter from postgresql.conf can be entered here and will be delivered by the operator to all nodes of the cluster accordingly.\nYou can find more information about the parameters in the PostgreSQL documentation\npatroni The patroni object contains numerous options for customising the patroni-setu, and the pg_hba.conf is also configured here. A complete list of all available elements can be found here.\nThe most important elements include\npg_hba - pg_hba.conf slots synchronous_mode - enables synchronous mode in the cluster. The default is set to false maximum_lag_on_failover - Specifies the maximum lag so that the pod is still considered healthy in the event of a failover. failsafe_mode Allows you to cancel the downgrading of the leader if all cluster members can be reached via the Patroni Rest Api. You can find more information on this in the Patroni documentation pg_hba The pg_hba.conf contains all defined authentication rules for PostgreSQL.\nWhen customising this configuration, it is important that the entire version of pg_hba is written to the manifest. The current configuration can be read out in the database using table pg_hba_file_rules ;.\nFurther information can be found in the PostgreSQL documentation\nslots When using user-defined slots, for example for the use of CDC using Debezium, there are problems when interacting with Patroni, as the slot and its current status are not automatically synchronised to the replicas.\nIn the event of a failover, the client cannot start replication as both the entire slot and the information about the data that has already been synchronised are missing.\nTo resolve this problem, slots must be defined in the cluster manifest rather than in PostgreSQL.\nspec: patroni: slots: cdc-example: database: app_db plugin: pgoutput type: logical This example creates a logical replication slot with the name cdc-example within the app_db database and uses the pgoutput plugin for the slot.\nATTENTION: Slots are only synchronised from the leader/standby leader to the replicas. This means that using the slots read-only on the replicas will cause a problem in the event of a failover.\n","description":"Users who are already used to working with PostgreSQL from Baremetal or VMs are already familiar with the need for various files to configure PostgreSQL. These include\npostgresql.conf pg_hba.conf \u0026hellip; Although these files are available in the container, direct modification is not planned. As part of the declarative mode of operation of the operator, these files are defined via the operator. The modifying intervention within the container also represents a contradiction to the immutability of the container."},{"id":15,"href":"/CYBERTEC-pg-operator/first_cluster/","title":"Create a Cluster","parent":"CPO (CYBERTEC-PG-Operator)","content":"To set up a cluster, the implementation is based on a description, as with the other Kubernetes deplyoments. To do this, the operator uses a document of type postgresql.\nYou can also find the basic minimum specifications for a single-node cluster in our tutorial project on Github\nminimal Single-Node Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s HINT: Here you will find a complete overview of the available options within the cluster manifest.\nminimal High-Availability Cluster No more effort is required to create a High-Availablity cluster than for a Single-Node Cluster. Only the Cluster-Manifest needs to be modified slightly. The difference lies in the object numberOfInstances, which must be set \u0026gt; 1.\nYou can also find the basic minimum specifications for a High-Availability-Cluster cluster in our tutorial project on Github\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 2 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi You can either create a new cluster with the document or update an existing cluster with it. This makes it possible to scale the cluster up and down during operation.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 2m12s cluster-1-1 | 1/1 | Running | 0 | 50s ","description":"To set up a cluster, the implementation is based on a description, as with the other Kubernetes deplyoments. To do this, the operator uses a document of type postgresql.\nYou can also find the basic minimum specifications for a single-node cluster in our tutorial project on Github\nminimal Single-Node Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":16,"href":"/CYBERTEC-pg-operator/resources/","title":"Apply Ressources","parent":"CPO (CYBERTEC-PG-Operator)","content":"Kubernetes workloads are often deployed without a direct resource definition. This means that, apart from the limits specified by the administrators, the workloads can use the required resources of the worker node very dynamically.\nThe cluster manifest is used to define the Postgres pod resources via the typical resources objects.\nThere are basically two different definitions:\nrequests: Basic requirement and guaranteed by the worker node limits: maximum availability, allocation is increased dynamically if the worker node can provide the resources. For the planning of the cluster, a proper definition should be carried out in terms of the required hardware, which is then defined as requests. These resources are thus guaranteed to the cluster and are taken into account when deploying the pod. Accordingly, a pod can only be deployed on a worker if it can provide these resources. Any limits beyond this are not taken into account when deploying.\nThe unit of measurement should be taken into account when planning the necessary CPUs: cpu specifications are based on millicores\n1 cpu corresponds to 1 core 1 core corresponds to 1000 millicores (m) 1/2 core corresponds to 500 m resources: limits: cpu: 500m memory: 1Gi requests: cpu: 1000m memory: 1Gi This example corresponds to a guaranteed availability of half a core and 1 Gibibyte. However, if necessary and available, the container can use up to one core. The allocation takes place dynamically and for the required time.\nPods can be categorised into three Quality of Services (QoS) based on the defined information on the resources.\nBest-Effort: The containers of a pod have no resource information Burstable: A container of the pod has a memory or CPU requests or limits. Guaranteed: Each container of a pod has both cpu and memory requests and limits. In addition, the details of the respective limits correspond to the requests details If you would like more information and explanations, you can take a look at the Kubernetes documentation on QoS.\nWe generally recommend using the Guaranteed Status for PostgreSQL workloads. However, many users very successfully use the deviation of the CPU limit by factors such as 2. For example:\nresources: limits: cpu: 1000m memory: 1Gi requests: cpu: 2000m memory: 1Gi This is intended to create the possibility of additional CPU resources for sudden load peaks.\nHINT: The use of burstable definitions does not release you from a correct resource calculation, as limits resources are not guaranteed and therefore an undersupply can occur if the requests are not properly defined.\n","description":"Kubernetes workloads are often deployed without a direct resource definition. This means that, apart from the limits specified by the administrators, the workloads can use the required resources of the worker node very dynamically.\nThe cluster manifest is used to define the Postgres pod resources via the typical resources objects.\nThere are basically two different definitions:\nrequests: Basic requirement and guaranteed by the worker node limits: maximum availability, allocation is increased dynamically if the worker node can provide the resources."},{"id":17,"href":"/CYBERTEC-pg-operator/storage/","title":"Storage","parent":"CPO (CYBERTEC-PG-Operator)","content":"Storage is crucial for the performance of a database and is therefore a central element. As with systems based on bare metal or virtual machines, the same requirements apply to Kubernetes workloads, such as constant availability, good performance, consistency and durability.\nA basic distinction is made between local storage, which is directly connected to the worker node, and network storage, which is mounted on the worker node and thus made available to the pod.\nIn probably the vast majority of Kubernetes systems, network storage is used, for example from systems from hyperscalers or other cloud providers or own systems such as CEPH.\nWith network storage in particular, attention must be paid to performance in terms of throughput (speed and guaranteed IOPS) and, above all, latency. It is also important to ensure that the different volumes do not compete with each other in terms of load.\nPAY ATTENTION: Before using a CPO cluster, make sure that the storage is suitable for the intended use and provides the necessary performance. In addition, check the storage with benchmarks before use. We recommend the use of pgbench for this purpose.\nDefine Storage-Volume The storage is defined via the volume object and enables the size and storage class for the storage to be defined, among other things.\nspec: volume: size: 5Gi storageClass: default-provisioner ... The volume is currently used for both PG and WAL data. In future, there will be an optional option to create a separate WAL volume. Please check our roadmap\nPAY ATTENTION: Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nThe volume is currently used for both PG and WAL data. In future, there will be an optional option to create a separate WAL volume.\nExpanding Volume HINT: Kubernetes is able to forward requests to expand the storage to the storage system and enable the expand without the need to restart the container. However, this also requires the associated storage system and the driver used to support this. This information can be found in the storage class under the object: allowVolumeExpansion. A distinction must also be made between online and offline expand. The latter requires a restart of the pod. To do this, the pod must be deleted manually.\nTo Expand the Volume, the value of the object volume.size must be increased\nspec: volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Storage is crucial for the performance of a database and is therefore a central element. As with systems based on bare metal or virtual machines, the same requirements apply to Kubernetes workloads, such as constant availability, good performance, consistency and durability.\nA basic distinction is made between local storage, which is directly connected to the worker node, and network storage, which is mounted on the worker node and thus made available to the pod."},{"id":18,"href":"/CYBERTEC-pg-operator/db_users/","title":"Databases \u0026 Users","parent":"CPO (CYBERTEC-PG-Operator)","content":"CPO not only supports you in deploying your cluster, it also supports you in setting it up in terms of the database and users. CPO offers you three different options for this:\nCreate roles Create databases preapared databases Create Roles The creation of users is based on the definition of the user name and the definition of the required rights for this user. Available rights are\nsuperuser inherit login nologin createrole createdb replication bypassrls Unless explicitly defined via NOLOGIN, a created user automatically receives the LOGIN permission.\nspec: users: db_owner: - login - createdb appl_user: - login For each user created, CPO automatically creates a secret with username and password in the namespace of the cluster, which follows the following naming convention: [USERNAME].[CLUSTERNAME].credentials.postgresql.cpo.opensource.cybertec.at\nIf the secrets for an application are to be stored in a different namespace, for example, it is necessary to define the setting enable_cross_namespace_secret as true in the operator configuration. You can find more information about the operator configuration here.\nThe namespace must then be written before the user name.\nspec: users: db_owner: - login - createdb app_namespace.appl_user: - login Create Databases Databases are basically created in a very similar way to users. The definition is based on the database name and the database owner.\nspec: users: db_owner: - login - createdb app_namespace.appl_user: - login databases; app_db: app_namespace.appl_user HINT: Be aware that the user name must be defined for the database owner in the same way as it is done in the users object.\nPrepared Databases The preparedDatabases object is available for a much more extensive setup of databases and users. In addition to the creation of databases and users, this also enables the creation of schemas and extensions. A more detailed rights management is also available.\nDatabases and Schema Creating the preparedDatabases object already creates a database whose name is based on the cluster name. preparedDatabases: {}\nHINT: For the database name, - is replaced with _ in the cluster name\nTo create your own database names and elements such as schemas and extensions within the database, an object must be created within preparedDatabases for each database.\nspec: preparedDatabases: appl_db: extensions: dblink: public schemas: data: {} This example creates a database with the name appl_db and creates a schema with the name data in it, as well as creating the dblink extension in the schema public.\nManagement of users and Permissions For rights management, we distinguish between NOLOGIN roles and LOGIN roles. Users have login rights and inherit the other rights from the NOLOGIN role.\nNoLogin roles (defaultRoles) The roles are created if defaultroles is not explicitly set to false.\nspec: preparedDatabases: appl_db: extensions: dblink: public schemas: data: {} This creates roles for the schema owner, writer and reader\nLogin roles (defaultUsers) The roles described in the previous paragraph can be assigned to LOGIN roles via the users section in the manifest. Optionally, the Postgres operator can also create standard LOGIN roles for the database and each individual schema. These roles are given the suffix _user and inherit all rights from their NOLOGIN counterparts. Therefore, you cannot set defaultRoles to false and activate defaultUsers at the same time.\nspec: preparedDatabases: appl_db: defaultUsers: true extensions: dblink: public schemas: data: {} history: defaultRoles: true defaultUsers: false This example creates the following users and inheritances\nRole name Attributes inherits from appl_db_owner Cannot login appl_db_reader,appl_db_owner,appl_data_owner,\u0026hellip; appl_db_owner_user appl_db_owner appl_db_reader Cannot login appl_db_reader_user appl_db_reader appl_db_writer Cannot login appl_db_reader appl_db_writer_user appl_db_writer appl_db_data_owner Cannot login appl_db_data_reader,appl_db_data_writer appl_db_data_reader Cannot login appl_db_data_writer Cannot login appl_db_data_reader appl_db_history_owner Cannot login appl_db_history_reader,appl_db_history_writer appl_db_history_reader Cannot login appl_db_history_writer Cannot login appl_db_history_reader Default access permissions are also defined for LOGIN roles when databases and schemas are created. This means that they are not currently set if defaultUsers (or defaultRoles for schemas) are activated at a later time.\nUser Secrets For each user created by cpo with LOGIN permissions, the operator also creates a secret with username and password, as with the creation of roles via the users object.\n","description":"CPO not only supports you in deploying your cluster, it also supports you in setting it up in terms of the database and users. CPO offers you three different options for this:\nCreate roles Create databases preapared databases Create Roles The creation of users is based on the definition of the user name and the definition of the required rights for this user. Available rights are\nsuperuser inherit login nologin createrole createdb replication bypassrls Unless explicitly defined via NOLOGIN, a created user automatically receives the LOGIN permission."},{"id":19,"href":"/CYBERTEC-pg-operator/edit_cluster/","title":"Modify Cluster","parent":"CPO (CYBERTEC-PG-Operator)","content":"Starting with the Single-Node-Cluster from the previous section, we want to modify the Instance a bit to see.\nCPU and Memory spec: resources: limits: cpu: 1000m memory: 500Mi requests: cpu: 500m memory: 500mi Based on the ressources-Definiton we\u0026rsquo;re able to modify the reserved Hardware (requests) and the limits, which allows use to consume more than the reserved definitons if the k8s-worker has this hardware available. There are some Restrictions when modifiying the limits-section. Because of the behaviour of Databases we should never define a diff between requests.memory and limits.memory. A Database is after some time using all available Memory, for Cache and other things. Limits are optional and the worker node can force them back. forcing back memory will create big problems inside a database like creating corruption, forcing OutOfMemory-Killer and so on. CPU on the other side is a ressource we can use inside the limits definiton to allow our database using more cpu if needed and available.\nSidecars Sidecars are further Containers running on the same Pod as the Database. We can use them for serveral different Jobs. The Operator allows us to define them directly inside the Cluster-Manifest.\nspec: sidecars: - name: \u0026#34;telegraf-sidecar\u0026#34; image: \u0026#34;telegraf:latest\u0026#34; ports: - name: metrics containerPort: 8094 protocol: TCP resources: limits: cpu: 500m memory: 500Mi requests: cpu: 100m memory: 100Mi env: - name: \u0026#34;USEFUL_VAR\u0026#34; value: \u0026#34;perhaps-true\u0026#34; This Example will add a second Container to our Pods. This will trigger a restart, which creates Downtime if you\u0026rsquo;re not running a HA-Cluster.\nInit-Containers We can exactly the same as for sidecars also for Init-Containers. The difference is, that a sidecar is running normally on a pod. An Init-Container will just run as first container when the pod is created and it will ends after his job is done. The \u0026ldquo;normal\u0026rdquo; Containers has to wait till all init-Containers finished their jobs and ended with a exit-status.\nspec: initContainers: - name: date image: busybox command: [ \u0026#34;/bin/date\u0026#34; ] TLS-Certificates One Startup the Containers will create a custom TLS-Certificate which allows creating tls-secured-connections to the Database. But this Certificates cannot verified, because the application has no information about the CA. Because of this the certificates are no protection against MITM-Attacks. You\u0026rsquo;re able to configure your own Certificates and CA to ensure, that you can use secured and verified connections between your application and your database.\nspec: tls: secretName: \u0026#34;\u0026#34; # should correspond to a Kubernetes Secret resource to load certificateFile: \u0026#34;tls.crt\u0026#34; privateKeyFile: \u0026#34;tls.key\u0026#34; caFile: \u0026#34;\u0026#34; # optionally configure Postgres with a CA certificate caSecretName: \u0026#34;\u0026#34; # optionally the ca.crt can come from this secret instead. You need to store the needed values from tls.crt, tls.key and ca.crt in a secret and define the secrtetname inside the tls-object. if you want you can create a separate sercet just for the ca and use this secret for every cluster inside the Namespace. To get Information about creating Certificates and the secrets check the Tutorial in the additonal-Section or click here\nNode-Affinity Node-Affinity will ensure that the Cluster-pods only deployed on Kubernetes-Nodes which has the defined Labelkey and -Value\nspec: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: cpo operator: In values: - enabled This allowes you to use specific database-nodes in a mixed cluster for example. In the Example above the Cluster-Pods are just deployed on Nodes with the Key: cpo and the value: enabled So you\u0026rsquo;re able to seperate your Workload.\nPostgreSQL-Configuration Every Cluster will start with the default PostgreSQL-Configuration. Every Parameter can be overriden based in definitions inside the Cluster-Manifest. Therefore we just need a add the section parameters to the postgresql-Object\nspec: postgresql: version: 16 parameters: max_connections: \u0026#34;53\u0026#34; log_statement: \u0026#34;all\u0026#34; track_io_timing: \u0026#34;true\u0026#34; These Definitions will change the PostgreSQL-Configuration. Based on the needs of Parameter changes the Pods may needs a restart, which creates a Downtime if its not a HA-Cluster. You can check Parameters and allowed Values on this Sources to ensure a correct Value.\nPostgreSQL Documentation PostgreSQL.org PostgreSQLco.nf ","description":"Starting with the Single-Node-Cluster from the previous section, we want to modify the Instance a bit to see.\nCPU and Memory spec: resources: limits: cpu: 1000m memory: 500Mi requests: cpu: 500m memory: 500mi Based on the ressources-Definiton we\u0026rsquo;re able to modify the reserved Hardware (requests) and the limits, which allows use to consume more than the reserved definitons if the k8s-worker has this hardware available. There are some Restrictions when modifiying the limits-section."},{"id":20,"href":"/CYBERTEC-pg-operator/ha_cluster/","title":"High Availability","parent":"CPO (CYBERTEC-PG-Operator)","content":"To ensure continiues productive usage you can create a HA-Cluster or modify a Single-Node-Cluster to a HA-Cluster. The needed changes are less complicated\nspec: numberOfInstances: 2 # or more The example above will create a HA-Cluster based on two Nodes.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 54s cluster-1-1 | 1/1 | Running | 0 | 31s ","description":"To ensure continiues productive usage you can create a HA-Cluster or modify a Single-Node-Cluster to a HA-Cluster. The needed changes are less complicated\nspec: numberOfInstances: 2 # or more The example above will create a HA-Cluster based on two Nodes.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 54s cluster-1-1 | 1/1 | Running | 0 | 31s "},{"id":21,"href":"/CYBERTEC-pg-operator/config_cluster/","title":"PostgreSQL Configuration","parent":"CPO (CYBERTEC-PG-Operator)","content":"Users who are already used to working with PostgreSQL from Baremetal or VMs are already familiar with the need for various files to configure PostgreSQL. These include\npostgresql.conf pg_hba.conf \u0026hellip; Although these files are available in the container, direct modification is not planned. As part of the declarative mode of operation of the operator, these files are defined via the operator. The modifying intervention within the container also represents a contradiction to the immutability of the container.\nFor these reasons, the operator provides a way to make adjustments to the various files, from PostgreSQL to Patroni.\nWe differentiate between two main objects in the cluster manifest:\npostgresql with the child objects version and parameters patroni with objects for the pg_hab, slots and much more postgresql The postgresql object consists of the following elements:\nversion - allows you to select the major version of PostgreSQL used. parameters- enables the postgresql.conf to be changed spec: postgresql: parameters: shared_preload_libraries: \u0026#39;pg_stat_statements,pgnodemx, timescaledb\u0026#39; shared_buffers: \u0026#39;512MB\u0026#39; version: \u0026#39;16\u0026#39; Any known PostgreSQL parameter from postgresql.conf can be entered here and will be delivered by the operator to all nodes of the cluster accordingly.\nYou can find more information about the parameters in the PostgreSQL documentation\npatroni The patroni object contains numerous options for customising the patroni-setu, and the pg_hba.conf is also configured here. A complete list of all available elements can be found here.\nThe most important elements include\npg_hba - pg_hba.conf slots synchronous_mode - enables synchronous mode in the cluster. The default is set to false maximum_lag_on_failover - Specifies the maximum lag so that the pod is still considered healthy in the event of a failover. failsafe_mode Allows you to cancel the downgrading of the leader if all cluster members can be reached via the Patroni Rest Api. You can find more information on this in the Patroni documentation pg_hba The pg_hba.conf contains all defined authentication rules for PostgreSQL.\nWhen customising this configuration, it is important that the entire version of pg_hba is written to the manifest. The current configuration can be read out in the database using table pg_hba_file_rules ;.\nFurther information can be found in the PostgreSQL documentation\nslots When using user-defined slots, for example for the use of CDC using Debezium, there are problems when interacting with Patroni, as the slot and its current status are not automatically synchronised to the replicas.\nIn the event of a failover, the client cannot start replication as both the entire slot and the information about the data that has already been synchronised are missing.\nTo resolve this problem, slots must be defined in the cluster manifest rather than in PostgreSQL.\nspec: patroni: slots: cdc-example: database: app_db plugin: pgoutput type: logical This example creates a logical replication slot with the name cdc-example within the app_db database and uses the pgoutput plugin for the slot.\nATTENTION: Slots are only synchronised from the leader/standby leader to the replicas. This means that using the slots read-only on the replicas will cause a problem in the event of a failover.\n","description":"Users who are already used to working with PostgreSQL from Baremetal or VMs are already familiar with the need for various files to configure PostgreSQL. These include\npostgresql.conf pg_hba.conf \u0026hellip; Although these files are available in the container, direct modification is not planned. As part of the declarative mode of operation of the operator, these files are defined via the operator. The modifying intervention within the container also represents a contradiction to the immutability of the container."},{"id":22,"href":"/CYBERTEC-pg-operator/backup/","title":"Backup","parent":"CPO (CYBERTEC-PG-Operator)","content":"Backups are essential for databases. From broken storage to deployments gone wrong, backups often save the day. Starting with pg_dump, which was released in the late 1990s, to the archiving of WAL files (PostgreSQL 8.0 / 2005) and pg_basebackup (PostgreSQL 9.0 / 2010), PostgreSQL already offers built-in options for backups and restores based on logical and physical backups.\nBackups with pgBackRest CPO relies on pgBackRest as its backup solution, a tried-and-tested tool with extensive backup and restore options. The backup is based on two elements:\nSnapshots in the form of physical backups WAL archive: Continuous archiving of the WAL files Backups Backups represent a snapshot of the database in the form of pyhsical files. This contains all relevant information that PostgreSQL holds in its data folder. With pgBackRest it is possible to create different types of Backups:\nfull Snapshot: This captures and saves all files at the time of the backup Differential backup: Only captures all files that have been changed since the last full Backup Incremental backup: Only records the files that have been changed since the last backup (of any kind). When restoring using differential or incremental Backup, it is necessary to also use the previous Backup that provide the basis for the selected Backup.\nHINT: The choice of Backup types depends on factors such as the size of the database, the time available for backups and the restore.\nWAL-Archive The WAL (Write-Ahead-Log) refers to log files which record all changes to the database data before they are written to the actual files. The basic idea here is to guarantee the consistency and recoverability of the comitted data even in the event of failures.\nPostgreSQL normally cleans up or recycles the WAL files that are no longer required. By using WAL archiving, the WAL files are saved to a different location before this process so that they can be used for various activities in the future. These activities include\nProviding the WAL files for replicas to keep them up to date Restoring instances that have lost parts of the WAL files in the event of a failure and cannot return to a consistent state without them without losing data Point-In-Time-Recovery (PITR): In contrast to Backups, which map a fixed point in time, WAL files make it possible to jump dynamically to a desired point in time and restore the database to the closest available consistent data point HINT: WAL archiving is an indispensable tool for data availability, recoverability and the continuous availability of PostgreSQL.\nBackup your Cluster With pgBackRest, backups can be stored on different types of storage:\nBlock storage (PVC) S3 / S3-compatible storage Azure blob storage GCS Backups on PVC (PersistentVolumeClaim) When using block storage, the operator creates an additional pod that acts as a repo host. Based on a TLS connection, the repo host obtains the data for the Backup from the current primary of the cluster, which is compressed before being sent. WAL archives are pushed from the primary pod to the repo host.\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster namespace: cpo spec: backup: pgbackrest: image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container-dev:pgbackrest-16.3-1\u0026#39; repos: - name: repo1 schedule: full: 30 2 * * * storage: pvc volume: size: 15Gi storageClass: default global: repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count This example creates backups based on a repo host with a daily full Backup at 2:30 am. In addition, pgBackRest is instructed to keep a maximum of 7 full Backups. The oldest one is always removed when a new Backup is created.\nHINT: In addition, further configurations for pgBackRest can be defined in the global object. Information on possible configurations can be found in the pgBackRest documentation\nBackups on S3 pgBackRest can be used directly with AWS S3 or S3-compatible storage such as MinIO, Cloduian HyperStore or SwiftStack.\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster namespace: cpo spec: backup: pgbackrest: image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container-dev:pgbackrest-16.3-1\u0026#39; repos: - endpoint: \u0026#39;https://s3-zurich.cyberlink.cloud:443\u0026#39; name: repo1 region: zurich resource: cpo-cluster-bucket schedule: full: 30 2 * * * incr: \u0026#39;*/30 * * * *\u0026#39; storage: s3 configuration: secret: cluster-s3-credential global: repo1-path: /cluster/repo1/ repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count This example creates a backup in an S3 bucket. In addition to the above configurations, a secret is also required which contains the access data for the S3 storage. The name of the secret must be stored in the spec.backup.pgbackrest.configuration.secret object and the secret must be located in the same namespace as the cluster. Information required to address the S3 bucket:\nEndpoint: S3 api endpoint Region: Region of the bucket resource: Name of the bucket The secret must be defined as follows for the use of S3 storage:\nkind: Secret apiVersion: v1 metadata: name: cluster-s3-credential namespace: cpo stringData: s3.conf | [global] repo1-s3-key=YOUR_S3_KEY repo1-s3-key-secret=YOUR_S3_KEY_SECRET An example with a sercret generator is also available in the tutorials. Enter your access data in the s3.conf file and transfer the tutorial to your Kubernetes with kubectl apply -k cluster-tutorials/pgbackrest_with_s3/.\nEncrypt your backup client-side pgBackRest also allows you to encrypt your backups on the client side before uploading them. This is possible with any type of storage and is very easy to activate.\nFirstly, we need to define an encryption key. This must be specified separately for each repo and stored in the same secret that is defined in the spec.backup.pgbackrest.configuration.secret object.\nkind: Secret apiVersion: v1 metadata: name: cluster-s3-credential namespace: cpo stringData: s3.conf | [global] repo1-s3-key=YOUR_S3_KEY repo1-s3-key-secret=YOUR_S3_KEY_SECRET repo1-cipher-pass=YOUR_ENCRYPTION_KEY We also need to configure the type of encryption for pgBackRest. This is done via the cipher-type parameter, which must also be specified for each repo. You can find the available values for the parameter here\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster namespace: cpo spec: backup: pgbackrest: configuration: secret: cluster-s3-credential global: repo1-path: /cluster/repo1/ repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count repo1-cipher-type: aes-256-cbc image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container-dev:pgbackrest-16.3-1\u0026#39; repos: - endpoint: \u0026#39;https://s3-zurich.cyberlink.cloud:443\u0026#39; name: repo1 region: zurich resource: cpo-cluster-bucket schedule: full: 30 2 * * * incr: \u0026#39;*/30 * * * *\u0026#39; storage: s3 How a Backup works The operator creates a cronjob object on Kubernetes based on the defined times for automatic backups. This means that the Kubernetes core (CronJob Controller) will take care of processing the automatic backups and create a job and thus a pod at the appropriate time. The pod will send the backup command to the primary or, if block storage is used, to the repo host and monitor it. As soon as the backup is successfully completed, the pod stops with Completed and thus completes the job.\nkubectl get cronjobs --------------------------------------------------------------------------------------- NAME | SCHEDULE | SUSPEND | ACTIVE | LAST SCHEDULE | AGE pgbackrest-cluster-repo1-full | 30 2 * * * | False | 0 | 4h46m | 14h pgbackrest-cluster-repo1-incr | */30 * * * * | False | 1 | 81s | 106m kubectl get jobs ----------------------------------------------------------------------- NAME | COMPLETIONS | DURATION | AGE pgbackrest-cluster-repo1-full-28597110 | 1/1 | 52s | 140m pgbackrest-cluster-repo1-incr-28597365 | 1/1 | 2m37s | 32m pgbackrest-cluster-repo1-incr-28597380 | 1/1 | 2m38s | 17m pgbackrest-cluster-repo1-incr-28597395 | 0/1 | 2m3s | 2m3s If there are problems such as a timeout, the pod will stop with exit code 1 and thus indicate an error. In this case, a new pod will be created which will attempt to complete the backup. The maximum number of attempts is 6, so if the backup fails six times, the job is deemed to have failed and will not be attempted again until the next cronjob execution. The job pod log provides information about the problems.\nkubectl get pods ----------------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-0 | 2/2 | Running | 2 | 14h cluster-pgbackrest-repo-host-0 | 1/1 | Running | 0 | 107m pgbackrest-cluster-repo1-full-28597110-x8zpw | 0/1 | Completed | 0 | 143m pgbackrest-cluster-repo1-incr-28597365-7bb5l | 0/1 | Completed | 0 | 34m pgbackrest-cluster-repo1-incr-28597380-j76rr | 0/1 | Completed | 0 | 19m pgbackrest-cluster-repo1-incr-28597395-rh86t | 0/1 | Completed | 0 | 4m27s postgres-operator-66bbff5c54-5sjmk | 1/1 | Running | 0 | 47m How to check existing backups There are several ways to gain an insight into the current status of pgBackRest. One of these is to use pgBackRest within the container. This can be done both via the repo host and the Postgres pod.\npgbackrest via terminal (Repo-Host-Pod) kubectl exec cluster-5-pgbackrest-repo-host-0 --stdin --tty -- pgbackrest info stanza: db status: ok cipher: none db (current) wal archive min/max (16): 00000006000000000000005C/000000070000000000000092 full backup: 20240517-125730F timestamp start/stop: 2024-05-17 12:57:30+00 / 2024-05-17 12:57:41+00 wal start/stop: 00000007000000000000005E / 00000007000000000000005E database size: 22.9MB, database backup size: 22.9MB repo1: backup set size: 3MB, backup size: 3MB incr backup: 20240517-125730F_20240517-130003I timestamp start/stop: 2024-05-17 13:00:03+00 / 2024-05-17 13:00:05+00 wal start/stop: 000000070000000000000060 / 000000070000000000000060 database size: 22.9MB, database backup size: 904.3KB repo1: backup set size: 3MB, backup size: 149.4KB backup reference list: 20240517-125730F incr backup: 20240517-125730F_20240517-131503I timestamp start/stop: 2024-05-17 13:15:03+00 / 2024-05-17 13:15:04+00 wal start/stop: 000000070000000000000062 / 000000070000000000000062 database size: 22.9MB, database backup size: 24.3KB repo1: backup set size: 3MB, backup size: 2.9KB backup reference list: 20240517-125730F, 20240517-125730F_20240517-130003I pgbackrest via terminal (Postgres-Pod) kubectl exec cluster-5-0 --stdin --tty -- pgbackrest info Defaulted container \u0026#34;postgres\u0026#34; out of: postgres, postgres-exporter, pgbackrest-restore (init) stanza: db status: ok cipher: none db (current) wal archive min/max (16): 00000006000000000000005C/000000070000000000000092 full backup: 20240517-125730F timestamp start/stop: 2024-05-17 12:57:30+00 / 2024-05-17 12:57:41+00 wal start/stop: 00000007000000000000005E / 00000007000000000000005E database size: 22.9MB, database backup size: 22.9MB repo1: backup set size: 3MB, backup size: 3MB incr backup: 20240517-125730F_20240517-130003I timestamp start/stop: 2024-05-17 13:00:03+00 / 2024-05-17 13:00:05+00 wal start/stop: 000000070000000000000060 / 000000070000000000000060 database size: 22.9MB, database backup size: 904.3KB repo1: backup set size: 3MB, backup size: 149.4KB backup reference list: 20240517-125730F incr backup: 20240517-125730F_20240517-131503I timestamp start/stop: 2024-05-17 13:15:03+00 / 2024-05-17 13:15:04+00 wal start/stop: 000000070000000000000062 / 000000070000000000000062 database size: 22.9MB, database backup size: 24.3KB repo1: backup set size: 3MB, backup size: 2.9KB backup reference list: 20240517-125730F, 20240517-125730F_20240517-130003I There is the \u0026ldquo;normal\u0026rdquo; output, as well as the output format Json, which can be processed directly in the terminal.\nkubectl exec cluster-5-0 --stdin --tty -- pgbackrest info --output=json Check pgBackrest via Monitoring In addition to reading the status via the containers, pgBackRest can also be analysed and monitored via the monitoring stack. You can find information on setting up the monitoring stack and further information here.\n","description":"Backups are essential for databases. From broken storage to deployments gone wrong, backups often save the day. Starting with pg_dump, which was released in the late 1990s, to the archiving of WAL files (PostgreSQL 8.0 / 2005) and pg_basebackup (PostgreSQL 9.0 / 2010), PostgreSQL already offers built-in options for backups and restores based on logical and physical backups.\nBackups with pgBackRest CPO relies on pgBackRest as its backup solution, a tried-and-tested tool with extensive backup and restore options."},{"id":23,"href":"/CYBERTEC-pg-operator/restore/","title":"Restore","parent":"CPO (CYBERTEC-PG-Operator)","content":"Restore or recovery is the process of starting a PostgreSQL instance or a cluster based on a defined and existing backup. This can be just a Backup or a combination of a Backup and additional WAL files. The difference is that a Backup represents a fixed point in time, whereas the combination with WAL enables a point-in-time recovery(PITR).\nYou can find more information about backups here\nRescue my cluster CPO enables the restore based on an existing backup using pgBackRest. To do this, it needs the relevant information about the point in time or snapBackupshot to which it should restore and where the data for this comes from. As we have already provided the operator with all the information relating to the storage of backups in the previous chapter, it only needs the following information:\nid: Control variable, must be increased for each restore process type: What type of restore is required repo: Which repo the data should come from set: Specific Backup to restore - Check backup to see how to get the identifier HINT: To ensure that the operator does not repeat an already done restore, the defined object id in the restore section is saved by the operator, so the value of this id must be changed for a new restore.\nDetails for a Backup restore With this information, we define a fixed Backup from repo1 and that pgBackRest should stop at the end of the Backup\nrestore: id: \u0026#39;1\u0026#39; options: type: \u0026#39;immediate\u0026#39; set: \u0026#39;20240515-164100F\u0026#39; repo: \u0026#39;repo1\u0026#39; HINT: Without the specification --type=immediate, pgBackRest would then consume the entire WAL that is available and thus restore the last available consistent data point.\nDetails for a point-in-time recoery (PITR) We use this information to define a point-in-time recovery (PITR) and define the end point using a timestamp and the start point using a Backup specification. The latter is optional. Without this information, pgBackRest would automatically start at the last previous full Backup.\nrestore: id: \u0026#39;1\u0026#39; options: type: \u0026#39;time\u0026#39; set: \u0026#39;20240515-164100F\u0026#39; target: \u0026#39;2024-05-16 07:46:05.506817+00\u0026#39; repo: \u0026#39;1\u0026#39; HINT: --type=time indicates that it is a point-in-time recovery (PITR).\nExample in a cluster manifest apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-5 namespace: cpo spec: backup: pgbackrest: configuration: secret: cluster-pvc-credentials global: repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container-dev:pgbackrest-16.2-8-dev\u0026#39; repos: - name: repo1 schedule: full: 30 2 * * * storage: pvc volume: size: 1Gi restore: id: \u0026#39;1\u0026#39; options: type: \u0026#39;time\u0026#39; set: \u0026#39;20240515-164100F\u0026#39; target: \u0026#39;2024-05-16 07:46:05.506817+00\u0026#39; An example of this can also be found in our tutorials. For a point-in-time recovery (PITR) you can find it here.\nATTENTION: Incorrect information for the Backup or the timestamp can result in pgBackRest not being able to complete the restore successfully. In the event of an error, the information must be corrected and another restore must be started.\n","description":"Restore or recovery is the process of starting a PostgreSQL instance or a cluster based on a defined and existing backup. This can be just a Backup or a combination of a Backup and additional WAL files. The difference is that a Backup represents a fixed point in time, whereas the combination with WAL enables a point-in-time recovery(PITR).\nYou can find more information about backups here\nRescue my cluster CPO enables the restore based on an existing backup using pgBackRest."},{"id":24,"href":"/CYBERTEC-pg-operator/connection_pooler/","title":"connection pooler","parent":"CPO (CYBERTEC-PG-Operator)","content":"A connection pooler is a tool that acts as a proxy between the application and the database and enables the performance of the application to be improved and the load on the database to be reduced. The reason for this lies in the connection handling of PostgreSQL.\nHow PostgreSQL handles connection PostgreSQL use a new Process for every database-connection created by the postmaster. This process is handling the connection. On the positive side, this enables a stable connection and isolation, but it is not particularly efficient for short-lived connections due to the effort required to create them.\nHow Connection Pooling solves this problem With connection pooling, the application connects to the pooler, which in turn maintains a number of connections to the PostgreSQL database. This makes it possible to use the connections from the pooler to the database for a long time instead of short-lived connections and to recycle them accordingly.\nIn addition to utilising long-term connections, a ConnectionPooler also makes it possible to reduce the number of connections required to the database. For example, if you have 3 application nodes, each of which maintains 100 connections to the database at the same time, that would be 300 connections in total. The application usually does not even begin to utilise this number of connections. With the pgBouncer, this can be optimised so that the applications open the 300 connections to the pgBouncer, but the pgBouncer only generates 100 connections to PostgreSQL, for example, thus reducing the load by 2/3.\nHINT: It is important to correctly configure the bouncer and thus the connections to be created between pgBouncer and PostgreSQL so that enough connections are available for the workload.\nHow does this work with CPO CPO relies on pgBouncer, a popular and above all lightweight open source tool. pgBouncer manages individual user-database connections for each user used, which can be used immediately for incoming client connections.\nHow do I create a pooler for a cluster? connection_pooler.number_of_instances - How many instances of connection pooler to create. Default is 2 which is also the required minimum.\nconnection_pooler.schema - Database schema to create for credentials lookup function to be used by the connection pooler. Is is created in every database of the Postgres cluster. You can also choose an existing schema. Default schema is pooler.\nconnection_pooler.user - User to create for connection pooler to be able to connect to a database. You can also choose an existing role, but make sure it has the LOGIN privilege. Default role is pooler.\nconnection_pooler.image - Docker image to use for connection pooler deployment. Default: “registry.opensource.zalan.do/acid/pgbouncer”\nconnection_poole.max_db_connections - How many connections the pooler can max hold. This value is divided among the pooler pods. Default is 60 which will make up 30 connections per pod for the default setup with two instances.\nconnection_pooler.mode - Defines pooler mode. Available Value: session, transaction or statement. Default is transaction.\nconnection_pooler.resources - Hardware definition for the pooler pods\nenableConnectionPooler - Defines whether poolers for read/write access should be created based on the spec.connectionPooler definition.\nenableReplicaConnectionPooler- Defines whether poolers for read-only access should be created based on the spec.connectionPooler definition.\nspec: connectionPooler: mode: transaction numberOfInstances: 2 resources: limits: cpu: \u0026#39;1\u0026#39; memory: 100Mi requests: cpu: 500m memory: 100Mi schema: pooler user: pooler enableConnectionPooler: true enableReplicaConnectionPooler: true ","description":"A connection pooler is a tool that acts as a proxy between the application and the database and enables the performance of the application to be improved and the load on the database to be reduced. The reason for this lies in the connection handling of PostgreSQL.\nHow PostgreSQL handles connection PostgreSQL use a new Process for every database-connection created by the postmaster. This process is handling the connection. On the positive side, this enables a stable connection and isolation, but it is not particularly efficient for short-lived connections due to the effort required to create them."},{"id":25,"href":"/CYBERTEC-pg-operator/monitoring/","title":"Monitoring","parent":"CPO (CYBERTEC-PG-Operator)","content":"The CPO-Project has prepared severall Tools which allows to setup a Monitoring-Stack including Alerting and Metric-Viewer. These Stack is based on:\nPrometheus Alertmanager Grafana exporter-container CPO has prepared an own Exporter for the PostgreSQl-Pod which can used as a sidecar.\nSetting up the Monitoring Stack To setup the Monitoring-Stack we suggest that you create an own namespace and use the prepared kustomization file inside the Operator-Tutorials.\n$ kubectl create namespace cpo-monitoring namespace/cpo-monitoring created $ kubectl get pods -n cpo-monitoring No resources found in cpo-monitoring namespace. git clone https://github.com/cybertec-postgresql/CYBERTEC-operator-tutorial cd CYBERTEC-operator-tutorial/setup/monitoring # Hint: Please check if youn want to use a specific storage-class the file pvcs.yaml and add your storageclass on the commented part. Please ensure that you removed the comment-char. $ kubectl apply -n cpo-monitoring -k . serviceaccount/cpo-monitoring created serviceaccount/cpo-monitoring-tools created clusterrole.rbac.authorization.k8s.io/cpo-monitoring unchanged clusterrolebinding.rbac.authorization.k8s.io/cpo-monitoring unchanged configmap/alertmanager-config created configmap/alertmanager-rules-config created configmap/cpo-prometheus-cm created configmap/grafana-dashboards created configmap/grafana-datasources created secret/grafana-secret created service/cpo-monitoring-alertmanager created service/cpo-monitoring-grafana created service/cpo-monitoring-prometheus created persistentvolumeclaim/alertmanager-pvc created persistentvolumeclaim/grafana-pvc created persistentvolumeclaim/prometheus-pvc created deployment.apps/cpo-monitoring-alertmanager created deployment.apps/cpo-monitoring-grafana created deployment.apps/cpo-monitoring-prometheus created Hint: If you\u0026#39;re not running Openshift you will get a error like this: error: resource mapping not found for name: \u0026#34;grafana\u0026#34; namespace: \u0026#34;\u0026#34; from \u0026#34;.\u0026#34;: no matches for kind \u0026#34;Route\u0026#34; in version \u0026#34;route.openshift.io/v1\u0026#34; ensure CRDs are installed first You can ignore this, because it depends on an object with the type route which is part of Openshift. It is not needed replaced by ingress-rules or an loadbalancer-service. After installing the Monitoring-Stack we\u0026rsquo;re able to check the created pods inside the namespace\n$ kubectl get pods -n cpo-monitoring ---------------------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cpo-monitoring-alertmanager-5bb8bc79f7-8pdv4 | 1/1 | Running | 0 | 3m35s cpo-monitoring-grafana-7c7c4f787b-jbj2f | 1/1 | Running | 0 | 3m35s cpo-monitoring-prometheus-67969b757f-k26jd | 1/1 | Running | 0 | 3m35s The configuration of this monitoring-stack is based on severall configmaps which can be modified.\nPrometheus-Configuration Alertmanager-Configuration Grafana-Configuration Configure a PostgreSQL-Cluster to allow Prometheus to gather metrics To allow Prometheus to gather metrics from your cluster you need to do some small modfications on the Cluster-Manifest. We need to create the monitor-object for this:\nkubectl edit postgresqls.cpo.opensource.cybertec.at cluster-1 ... spec: ... monitor: image: docker.io/cybertecpostgresql/cybertec-pg-container:exporter-16.2-1 The Operator will add automatically the monitoring sidecar to your pods, create a new postgres-user and add some structure inside the postgres-database to enable everthing needed for the Monitoring. Also every Ressource of your Cluster will get a new label: cpo_monitoring_stack=true. This is needed for Prometheus to identify all clusters which should be added to the monitoring. Removing this label will stop Prometheus to gather data from this cluster.\nAfter changing your Cluster-Manifest the Pods needs to be recreated which is done by a rolling update. After this you can see that the pod has now more than just one container.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 2/2 | Running | 0 | 54s cluster-1-1 | 2/2 | Running | 0 | 31s You can check the logs to see that the exporter is working and with curl you can see the output of the exporter.\nkubectl logs cluster-1-0 -c postgres-exporter kubectl exec --stdin --tty cluster-1-0 -c postgres-exporter -- /bin/bash [exporter@cluster-1-0 /]# curl http://127.0.0.1:9187/metrics You can now setup a LoadBalancer-Service or create an Ingress-Rule to allow access von outside to the grafana. Alternativ you can use a port-forward.\nLoadBalancer or Nodeport Ingress-Rule Port-Forwarding $ kubectl get pods -n cpo-monitoring ---------------------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cpo-monitoring-alertmanager-5bb8bc79f7-8pdv4 | 1/1 | Running | 0 | 6m42s cpo-monitoring-grafana-7c7c4f787b-jbj2f | 1/1 | Running | 0 | 6m42s cpo-monitoring-prometheus-67969b757f-k26jd | 1/1 | Running | 0 | 6m42s $ kubectl port-forward cpo-monitoring-grafana-7c7c4f787b-jbj2f -n cpo-monitoring 9000:9000 Forwarding from 127.0.0.1:9000 -\u0026gt; 9000 Forwarding from [::1]:9000 -\u0026gt; 9000 Call http://localhost:9000 in the Browser\nUse a Route (Openshift only) kubectl get route -n cpo-monitoring Use the Route-Adress to access Grafana\n","description":"The CPO-Project has prepared severall Tools which allows to setup a Monitoring-Stack including Alerting and Metric-Viewer. These Stack is based on:\nPrometheus Alertmanager Grafana exporter-container CPO has prepared an own Exporter for the PostgreSQl-Pod which can used as a sidecar.\nSetting up the Monitoring Stack To setup the Monitoring-Stack we suggest that you create an own namespace and use the prepared kustomization file inside the Operator-Tutorials.\n$ kubectl create namespace cpo-monitoring namespace/cpo-monitoring created $ kubectl get pods -n cpo-monitoring No resources found in cpo-monitoring namespace."},{"id":26,"href":"/CYBERTEC-pg-operator/cluster_upgrade/","title":"Major-Upgrade","parent":"CPO (CYBERTEC-PG-Operator)","content":"CPO enables the use of the in-place upgrade, which makes it possible to upgrade a cluster to a new PG major. For this purpose, pg_upgrade is used in the background.\nPAY ATTENTION: Note that an in-place upgrade generates both a pod restore in the form of a rolling update and an operational interruption of the cluster during the actual execution of the restore.\nHow does the upgrade work? Preconditions: Pod restart - Use the rolling update strategy to replace all pods based on the new ENV PGVERSION with the version you want to update to. Check - Check that the new PGVERSION is larger than the previously used one. Check whether the new PGVERSION is larger than the previously used one and the maintenance mode of the cluster must be deactivated. In addition, the replicas should not have a high lag. Preliminary checks use initdb to prepare a new data_dir (data_new) based on the new PGVERSION. check the upgrade possibility with pg_upgrade --check HINT: If one of the steps is aborted, a cleanup is performed\nPrepare the Upgrade remove dependencies that can cause problems. For example, the extensions pg_stat_statements and pgaudit. activate the maintenance mode of the cluster terminate PostgreSQL in an orderly manner check pg_controldata for the checkpoint position and wait until all replicas apply the latest checkpoint location use port 5432 for rsyncd and start it Start the Upgrade Call pg_upgrade -k to start the Upgrade ATTENTION if the process failed, we need to rollback, if it was sucessful we\u0026rsquo;re reaching the point of no return\nRename the directories. data -\u0026gt; data_old and data_new -\u0026gt; data Update the Patroni.config (postgres.yml) Call Checkpoint on every replica and trigger rsync on the Replicas Wait for Replicas to complete rsxnc. Timeout: 300 Stop rsyncd on Primary and remove ininitialize key from DCS, because its based on the old sysid Start Patroni on the Primary and start the postgres locally Reset custom staticstics, warmup the Memory and start Analyze in stages in separate threads Wait for every Replica to become ready Disable the maintenance mode for the Cluster Restore custom statistics, analyze these tables and restore dropped objetcs from Prepare the upgrade Completion of the upgrade Drop directory data_old Trigger new Backup How a rollback is working? Stop rsynd if its running Disable the maintenance mode for the Cluster Drop directory data_new How to trigger a In-Place-Upgrade with cpo? spec: postgresql: version: \u0026#34;16\u0026#34; To trigger an In-Place-Upgrade you have just to increase the parameter spec.postgresql.version. If you choose a valid number the Operator will start with the prozedure, described above. If you choosse a not allowed value, you will give an error and if you decrease the value, the operator will just ignore it with the following log-Entry.\nOperator-Log ","description":"CPO enables the use of the in-place upgrade, which makes it possible to upgrade a cluster to a new PG major. For this purpose, pg_upgrade is used in the background.\nPAY ATTENTION: Note that an in-place upgrade generates both a pod restore in the form of a rolling update and an operational interruption of the cluster during the actual execution of the restore.\nHow does the upgrade work? Preconditions: Pod restart - Use the rolling update strategy to replace all pods based on the new ENV PGVERSION with the version you want to update to."},{"id":27,"href":"/CYBERTEC-pg-operator/crd/","title":"References","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":28,"href":"/CYBERTEC-pg-operator/release_notes/","title":"Release-Notes","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":29,"href":"/CYBERTEC-pg-operator/release_notes/release_notes/","title":"Release Notes","parent":"Release-Notes","content":" 0.7.0 Features Monitoring-Sidecar integrated via CRD Start with Monitoring Password-Hash per default set to scram-sha-256 pgBackRest with blockstorage using RepoHost Internal Certification-Management for RepoHost-Certificates Compatible with PG17Beta2 Changes API Change acid.zalan.do is replaced by cpo.opensource.cybertec.at - If you\u0026rsquo;re updating your Operator from previous Versions, please check this HowTo Migrate to new API Patroni-Compatibility has increased to Version 3.3.2 pgBackRest-Compatbility has increased to Version 2.52.1 Revision of the restore process Revision of the backup jobs Operator now using Rocky9 as Baseimage Updates Go-Package to 1.22.5 Fixes PDB Bug fixed - Single-Node Clusters are not creating PDBs anymore which can break Kubernetes-Update Wrong Templates inside Cronjobs fixed Supported Versions PG: 13 - 16 \u0026amp; 17Beta2 Patroni: 3.3.2 pgBackRest: 2.52.1 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.6.1 Release with fixes\nFixes Backup-Pod now runs with \u0026ldquo;best-effort\u0026rdquo; resource definition Der Init-Container für die Wiederherstellung verwendet nun die gleiche Ressource-Definition wie der Datenbank-Container, wenn es keine spezifische Definition im Cluster-Manifest gibt (spec.backup.pgbackrest.resources) Software-Versions PostgreSQL: 15.3 14.8, 13.11, 12.15 Patroni: 3.0.4 pgBackRest: 2.47 OS: Rocky-Linux 9.1 (4.18) 0.6.0 Release with some improvements and stabilisation measuresm\nFeatures Added Pod Topology Spread Constraints Added support for TDE based on the CYBERTEC PostgreSQL Enterprise Images (Licensed Container Suite) Software-Versions PostgreSQL: 15.3 14.8, 13.11, 12.15 Patroni: 3.0.4 pgBackRest: 2.47 OS: Rocky-Linux 9.1 (4.18) 0.5.0 Release with new Software-Updates and some internal Improvements\nFeatures Updated to Zalando Operator 1.9 Fixes internal Problems with Cronjobs updates for some API-Definitions Software-Versions PostgreSQL: 15.2 14.7, 13.10, 12.14 Patroni: 3.0.2 pgBackRest: 2.45 OS: Rocky-Linux 9.1 (4.18) 0.3.0 Release with some improvements and stabilisation measuresm\nFixes missing pgbackrest_restore configmap fixed Software-Versions PostgreSQL: 15.1 14.7, 13.9, 12.13, 11.18 and 10.23 Patroni: 3.0.1 pgBackRest: 2.44 OS: Rocky-Linux 9.1 (4.18) 0.1.0 Initial Release as a Fork of the Zalando-Operator\nFeatures Added Support for pgBackRest (PoC-State) Stanza-create and Initial-Backup are executed automatically Schedule automatic updates (Full/Incremental/Differential-Backup) Securely store backups on AWS S3 and S3-compatible storage Software-Versions PostgreSQL: 14.6, 13.9, 12.13, 11.18 and 10.23 Patroni: 2.4.1 pgBackRest: 2.42 OS: Rocky-Linux 9.0 (4.18) ","description":"0.7.0 Features Monitoring-Sidecar integrated via CRD Start with Monitoring Password-Hash per default set to scram-sha-256 pgBackRest with blockstorage using RepoHost Internal Certification-Management for RepoHost-Certificates Compatible with PG17Beta2 Changes API Change acid.zalan.do is replaced by cpo.opensource.cybertec.at - If you\u0026rsquo;re updating your Operator from previous Versions, please check this HowTo Migrate to new API Patroni-Compatibility has increased to Version 3.3.2 pgBackRest-Compatbility has increased to Version 2.52.1 Revision of the restore process Revision of the backup jobs Operator now using Rocky9 as Baseimage Updates Go-Package to 1."},{"id":30,"href":"/CYBERTEC-pg-operator/categories/","title":"Categories","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":31,"href":"/CYBERTEC-pg-operator/tags/","title":"Tags","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""}]