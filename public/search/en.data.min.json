[{"id":0,"href":"/CYBERTEC-pg-operator/","title":"CPO (CYBERTEC-PG-Operator)","parent":"","content":"Current Release: 0.7.0 (xx.xx.xxxx) Release Notes\nCPO (CYBERTEC PG Operator) allows you to create and run PostgreSQL clusters on Kubernetes.\nThe operator reduces your efforts and simplifies the administration of your PostgreSQL clusters so that you can concentrate on other things.\nThe following features characterise our operator:\nDeclarative mode of operation Takes over all the necessary steps for setting up and managing the PG cluster. Integrated backup solution, automatic backups and very easy restore (snapshot \u0026amp; PITR) Rolling update procedure for adjustments to the pods and minor updates Major upgrade with minimum interruption time Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing CPO is tested on the following platforms:\nKubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 Rancher AWS EKS Azure AKS Google GKE Furthermore, CPO is basically executable on any CSCF-certified Kubernetes platform.\n","description":"Current Release: 0.7.0 (xx.xx.xxxx) Release Notes\nCPO (CYBERTEC PG Operator) allows you to create and run PostgreSQL clusters on Kubernetes.\nThe operator reduces your efforts and simplifies the administration of your PostgreSQL clusters so that you can concentrate on other things.\nThe following features characterise our operator:\nDeclarative mode of operation Takes over all the necessary steps for setting up and managing the PG cluster. Integrated backup solution, automatic backups and very easy restore (snapshot \u0026amp; PITR) Rolling update procedure for adjustments to the pods and minor updates Major upgrade with minimum interruption time Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing CPO is tested on the following platforms:"},{"id":1,"href":"/CYBERTEC-pg-operator/documentation/cluster/ha-cluster/","title":"High Availability Cluster (HA)","parent":"Documentation","content":"To ensure continiues productive usage you can create a HA-Cluster or modify a Single-Node-Cluster to a HA-Cluster. The needed changes are less complicated\nspec: numberOfInstances: 2 # or more The example above will create a HA-Cluster based on two Nodes.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 54s cluster-1-1 | 1/1 | Running | 0 | 31s ","description":"To ensure continiues productive usage you can create a HA-Cluster or modify a Single-Node-Cluster to a HA-Cluster. The needed changes are less complicated\nspec: numberOfInstances: 2 # or more The example above will create a HA-Cluster based on two Nodes.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 54s cluster-1-1 | 1/1 | Running | 0 | 31s "},{"id":2,"href":"/CYBERTEC-pg-operator/documentation/cluster/modify-cluster/","title":"Modify Cluster","parent":"Documentation","content":"Starting with the Single-Node-Cluster from the previous section, we want to modify the Instance a bit to see.\nCPU and Memory spec: resources: limits: cpu: 1000m memory: 500Mi requests: cpu: 500m memory: 500mi Based on the ressources-Definiton we\u0026rsquo;re able to modify the reserved Hardware (requests) and the limits, which allows use to consume more than the reserved definitons if the k8s-worker has this hardware available. There are some Restrictions when modifiying the limits-section. Because of the behaviour of Databases we should never define a diff between requests.memory and limits.memory. A Database is after some time using all available Memory, for Cache and other things. Limits are optional and the worker node can force them back. forcing back memory will create big problems inside a database like creating corruption, forcing OutOfMemory-Killer and so on. CPU on the other side is a ressource we can use inside the limits definiton to allow our database using more cpu if needed and available.\nSidecars Sidecars are further Containers running on the same Pod as the Database. We can use them for serveral different Jobs. The Operator allows us to define them directly inside the Cluster-Manifest.\nspec: sidecars: - name: \u0026#34;telegraf-sidecar\u0026#34; image: \u0026#34;telegraf:latest\u0026#34; ports: - name: metrics containerPort: 8094 protocol: TCP resources: limits: cpu: 500m memory: 500Mi requests: cpu: 100m memory: 100Mi env: - name: \u0026#34;USEFUL_VAR\u0026#34; value: \u0026#34;perhaps-true\u0026#34; This Example will add a second Container to our Pods. This will trigger a restart, which creates Downtime if you\u0026rsquo;re not running a HA-Cluster.\nInit-Containers We can exactly the same as for sidecars also for Init-Containers. The difference is, that a sidecar is running normally on a pod. An Init-Container will just run as first container when the pod is created and it will ends after his job is done. The \u0026ldquo;normal\u0026rdquo; Containers has to wait till all init-Containers finished their jobs and ended with a exit-status.\nspec: initContainers: - name: date image: busybox command: [ \u0026#34;/bin/date\u0026#34; ] TLS-Certificates One Startup the Containers will create a custom TLS-Certificate which allows creating tls-secured-connections to the Database. But this Certificates cannot verified, because the application has no information about the CA. Because of this the certificates are no protection against MITM-Attacks. You\u0026rsquo;re able to configure your own Certificates and CA to ensure, that you can use secured and verified connections between your application and your database.\nspec: tls: secretName: \u0026#34;\u0026#34; # should correspond to a Kubernetes Secret resource to load certificateFile: \u0026#34;tls.crt\u0026#34; privateKeyFile: \u0026#34;tls.key\u0026#34; caFile: \u0026#34;\u0026#34; # optionally configure Postgres with a CA certificate caSecretName: \u0026#34;\u0026#34; # optionally the ca.crt can come from this secret instead. You need to store the needed values from tls.crt, tls.key and ca.crt in a secret and define the secrtetname inside the tls-object. if you want you can create a separate sercet just for the ca and use this secret for every cluster inside the Namespace. To get Information about creating Certificates and the secrets check the Tutorial in the additonal-Section or click here\nNode-Affinity Node-Affinity will ensure that the Cluster-pods only deployed on Kubernetes-Nodes which has the defined Labelkey and -Value\nspec: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: cpo operator: In values: - enabled This allowes you to use specific database-nodes in a mixed cluster for example. In the Example above the Cluster-Pods are just deployed on Nodes with the Key: cpo and the value: enabled So you\u0026rsquo;re able to seperate your Workload.\nPostgreSQL-Configuration Every Cluster will start with the default PostgreSQL-Configuration. Every Parameter can be overriden based in definitions inside the Cluster-Manifest. Therefore we just need a add the section parameters to the postgresql-Object\nspec: postgresql: version: 16 parameters: max_connections: \u0026#34;53\u0026#34; log_statement: \u0026#34;all\u0026#34; track_io_timing: \u0026#34;true\u0026#34; These Definitions will change the PostgreSQL-Configuration. Based on the needs of Parameter changes the Pods may needs a restart, which creates a Downtime if its not a HA-Cluster.\n","description":"Starting with the Single-Node-Cluster from the previous section, we want to modify the Instance a bit to see.\nCPU and Memory spec: resources: limits: cpu: 1000m memory: 500Mi requests: cpu: 500m memory: 500mi Based on the ressources-Definiton we\u0026rsquo;re able to modify the reserved Hardware (requests) and the limits, which allows use to consume more than the reserved definitons if the k8s-worker has this hardware available. There are some Restrictions when modifiying the limits-section."},{"id":3,"href":"/CYBERTEC-pg-operator/documentation/cluster/single-cluster/","title":"Single Cluster","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":4,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/configuration/","title":"Configuration","parent":"Documentation","content":"","description":""},{"id":5,"href":"/CYBERTEC-pg-operator/documentation/crd/crd-postgresql/","title":"CRD: postgresql","parent":"Documentation","content":" postgresql Name Type required Description apiVersion string true acid.zalando.do/v1 kind string true metadata object true spec object true status object false spec Name Type required Description backup object false Enables the definition of a customised backup solution for the cluster teamId string true name of the team the cluster belongs to numberOfInstances Int true Number of nodes of the cluster dockerImages string false Define a custom image to override the default schedulerName string false Define a custom Name to override the default spiloRunAsUser string false Define an User id which should be used for the pods spiloRunAsGroup string false Define an Group id which should be used for the pods spiloFSGroup string false Persistent Volumes for the pods in the StatefulSet will be owned and writable by the group ID specified. enableMasterLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres primary enableMasterPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the primary ConnectionPooler enableReplicaLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres replicas enableReplicaPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the Replica-ConnectionPooler allowedSourceRange string false Defines the range of IP networks (in CIDR-notation). The corresponding load balancer is accessible only to the networks defined by this parameter. users map false a map of usernames to user flags for the users that should be created in the cluster by the operator usersWithSecretRotation list false list of users to enable credential rotation in K8s secrets. The rotation interval can only be configured globally. usersWithInPlaceSecretRotation list false list of users to enable in-place password rotation in K8s secrets. The rotation interval can only be configured globally. databases map false a map of databases that should be created in the cluster by the operator tolerations list false a list of tolerations that apply to the cluster pods. Each element of that list is a dictionary with the following fields: key, operator, value, effect and tolerationSeconds podPriorityClassName string false a name of the priority class that should be assigned to the cluster pods. If not set then the default priority class is taken. The priority class itself must be defined in advance podAnnotations map false A map of key value pairs that gets attached as annotations to each pod created for the database. ServiceAnnotations map false A map of key value pairs that gets attached as annotations to each Service created for the database. enableShmVolume boolean false Start a database pod without limitations on shm memory. By default Docker limit /dev/shm to 64M (see e.g. the docker issue, which could be not enough if PostgreSQL uses parallel workers heavily. If this option is present and value is true, to the target database pod will be mounted a new tmpfs volume to remove this limitation. enableConnectionPooler boolean false creates a ConnectionPooler for the primary Database enableReplicaConnectionPooler boolean false creates a ConnectionPooler for the replica Databases enableLogicalBackup boolean false Enable logical Backups for this Cluster (Stored on S3) - s3-configuration for Operator is needed (Not for pgBackRest) logicalBackupSchedule string false Schedule for the logical backup K8s cron job. (Not for pgBackRest) additionalVolumes list false List of additional volumes to mount in each container of the statefulset pod. Each item must contain a name, mountPath, and volumeSource which is a kubernetes volumeSource. It allows you to mount existing PersistentVolumeClaims, ConfigMaps and Secrets inside the StatefulSet. back to parent\nbackup Name Type required Description pgbackrest object false Enables the definition of a pgbackrest-setup for the cluster back to parent\npgbackrest Name Type required Description configuration object false Enables the definition of a pgbackrest-setup for the cluster global object false images string true repos array true resources: object false Resource definition (limits.cpu, limits.memory \u0026amp; requests.cpu \u0026amp; requests.memory) back to parent\nconfiguration Name Type required Description secret object false Secretname with the contained S3 credentials (AccessKey \u0026amp; SecretAccessKey) (Note: must be placed in the same namespace as the cluster) protection object false Enable Protection-Options back to parent\nprotection Name Type required Description restore boolean false A restore is ignored as long as this option is set to true. back to parent\nrepos Name Type required Description name string true Name of the repository Required:Repo[1-4] storage string true Defines the used backup-storage (Choose from List: local,s3,blob,gcs) Currently s3 only! resource string true Bucket-/Instance-/Storage- or PVC-Name endpoint string false The Endpoint for the choosen Storage (Not required for local storage) region string false Region for the choosen Storage (S3 only) schedule string false Object for defining automatic backups back to parent\nschedule Name Type required Description full string false (Cron-Syntax) Define full backup incr string false (Cron-Syntax) Define incremental backup diff string false (Cron-Syntax) Define differential backup back to parent\nstatus Name Type required Description PostgresClusterStatus string false Shows the cluster status. Filled by the Operator back to parent\n","description":"postgresql Name Type required Description apiVersion string true acid.zalando.do/v1 kind string true metadata object true spec object true status object false spec Name Type required Description backup object false Enables the definition of a customised backup solution for the cluster teamId string true name of the team the cluster belongs to numberOfInstances Int true Number of nodes of the cluster dockerImages string false Define a custom image to override the default schedulerName string false Define a custom Name to override the default spiloRunAsUser string false Define an User id which should be used for the pods spiloRunAsGroup string false Define an Group id which should be used for the pods spiloFSGroup string false Persistent Volumes for the pods in the StatefulSet will be owned and writable by the group ID specified."},{"id":6,"href":"/CYBERTEC-pg-operator/documentation/","title":"Documentation","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":7,"href":"/CYBERTEC-pg-operator/documentation/cluster/","title":"Documentation","parent":"Documentation","content":"","description":""},{"id":8,"href":"/CYBERTEC-pg-operator/documentation/crd/","title":"Documentation","parent":"Documentation","content":"","description":""},{"id":9,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/","title":"Documentation","parent":"Documentation","content":"","description":""},{"id":10,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/installation/","title":"Installation","parent":"Documentation","content":" Prerequisites For the installation you either need our CPO tutorial repository or you install CPO directly from our registry.\nException: Installation via Operatorhub (Openshift only)\nCPO-Tutorial-Repository To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials CPO-Registry Create Namespace # kubectl kubectl create namespace cpo # oc oc create namespace cpo Install CPO There are several ways to install CPO:\nUse Helm Use apply Use Operatorhub (On Openshift only) Helm You can check and change the value.yaml of the helm diagram under the path helm/operator/values.yaml By default, the operator is defined so that it is configured via crd-configuration. If you wish, you can change this to configmap. There are also some other default settings.\nhelm install -n cpo cpo helm/operator/. The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\nApply The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\nOperatorhub The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\n","description":"Prerequisites For the installation you either need our CPO tutorial repository or you install CPO directly from our registry.\nException: Installation via Operatorhub (Openshift only)\nCPO-Tutorial-Repository To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials CPO-Registry Create Namespace # kubectl kubectl create namespace cpo # oc oc create namespace cpo Install CPO There are several ways to install CPO:"},{"id":11,"href":"/CYBERTEC-pg-operator/documentation/crd/crd-operator-configurator/","title":"Operator-Configuration","parent":"Documentation","content":" Name Type default Description enable_crd_registration boolean true crd_categories string all enable_lazy_spilo_upgrade boolean false enable_pgversion_env_var boolean true enable_spilo_wal_path_combat boolean false etcd_host string kubernetes_use_configmaps boolean false docker_image string sidecars list enable_shm_volume boolean true workers int 8 max_instances int -1 min_instances int -1 resync_period string 30m repair_period string 5m set_memory_request_to_limit boolean false debug_logging boolean true enable_db_access boolean true spilo_privileged boolean false spilo_allow_privilege_escalation boolean true watched_namespace string * major-upgrade-specific Name Type default Description major_version_upgrade_mode string off major_version_upgrade_team_allow_list string minimal_major_version string 9.6 target_major_version string 14 aws-specific Name Type default Description wal_s3_bucket string log_s3_bucket string kube_iam_role string aws_region string additional_secret_mount string additional_secret_mount_path string enable_ebs_gp3_migration boolean enable_ebs_gp3_migration_max_size int logical-backup-specific Name Type default Description logical_backup_docker_image string logical_backup_google_application_credentials string logical_backup_job_prefix string logical_backup_provider string logical_backup_s3_access_key_id string logical_backup_s3_bucket string logical_backup_s3_endpoint string logical_backup_s3_region string logical_backup_s3_secret_access_key string logical_backup_s3_sse string logical_backup_s3_retention_time string logical_backup_schedule string (Cron-Syntax) team-api-specific Name Type default Description enable_teams_api string teams_api_url string teams_api_role_configuration string enable_team_superuser boolean team_admin_role boolean enable_admin_role_for_users boolean pam_role_name string pam_configuration string protected_role_names list postgres_superuser_teams string role_deletion_suffix string enable_team_member_deprecation boolean enable_postgres_team_crd boolean enable_postgres_team_crd_superusers boolean enable_team_id_clustername_prefix boolean ","description":"Name Type default Description enable_crd_registration boolean true crd_categories string all enable_lazy_spilo_upgrade boolean false enable_pgversion_env_var boolean true enable_spilo_wal_path_combat boolean false etcd_host string kubernetes_use_configmaps boolean false docker_image string sidecars list enable_shm_volume boolean true workers int 8 max_instances int -1 min_instances int -1 resync_period string 30m repair_period string 5m set_memory_request_to_limit boolean false debug_logging boolean true enable_db_access boolean true spilo_privileged boolean false spilo_allow_privilege_escalation boolean true watched_namespace string * major-upgrade-specific Name Type default Description major_version_upgrade_mode string off major_version_upgrade_team_allow_list string minimal_major_version string 9."},{"id":12,"href":"/CYBERTEC-pg-operator/documentation/tutorials/abc/","title":"Operator-Configuration","parent":"Documentation","content":"","description":""},{"id":13,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/level-2-2-2/","title":"Overview","parent":"Documentation","content":"","description":""},{"id":14,"href":"/CYBERTEC-pg-operator/documentation/how_to_use/","title":"Overview","parent":"Documentation","content":"","description":""},{"id":15,"href":"/CYBERTEC-pg-operator/documentation/level-2-3/","title":"Overview","parent":"Documentation","content":"","description":""},{"id":16,"href":"/CYBERTEC-pg-operator/level-1-1-overview/","title":"Overview","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":17,"href":"/CYBERTEC-pg-operator/level-1/","title":"Overview","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":18,"href":"/CYBERTEC-pg-operator/news/","title":"Overview","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":19,"href":"/CYBERTEC-pg-operator/overview/","title":"Overview","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":20,"href":"/CYBERTEC-pg-operator/documentation/quickstart/","title":"Quickstart","parent":"Documentation","content":"We can tell and document so much about our project but it seems you just want to get started. Let us show you the fastest way to use CPO.\nPreconditions git helm (optional) kubectl or oc Let\u0026rsquo;s start Step 1 - Preparations To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials Step 2 - Install the Operator Two options are available for the installation:\nInstallation via Helm-Chart Installation via apply Installation via Helm-Chart kubectl apply -k setup/namespace/. helm install cpo setup/helm/operator/ -n cpo Installation via apply kubectl apply -k setup/namespace/. kubectl apply -k setup/helm/operator/. -n cpo You can check if the operator pod is in operation.\nkubectl get pods -n cpo --selector=cpo.cybertec.at/pod/type=postgres-operator The result should look like this:\nNAME READY STATUS RESTARTS AGE postgres-operator-599688d948-fw8pw 1/1 Running 0 41s The operator is ready and the setup is complete. The next step is the creation of a Postgres cluster\nStep 3 - Create a Cluster To create a simple cluster, the following command is sufficient\nkubectl apply -k cluster-tutorials/single-cluster watch kubectl get pods --selector cluster-name=cluster-1,application=spilo The result should look like this:\nAlle 2.0s: kubectl get pods --selector cluster-name=cluster-1,application=spilo NAME READY STATUS RESTARTS AGE cluster-1-0 2/2 Running 0 28s cluster-1-1 0/2 PodInitializing 0 9s Step 4 - Connect to the Database Get your login information from the secret.\nkubectl get secret postgres.acid-cluster-1.credentials.postgresql.acid.zalan.do -o jsonpath=\u0026#39;{.data}\u0026#39; | jq \u0026#39;.|map_values(@base64d)\u0026#39; The result should look like this:\n{ \u0026#34;password\u0026#34;: \u0026#34;2rZG1Kx9asdHscswQGzff4Ru0xW6uasacy3GQ0sjdCH3wWr0kguUXUZek6dkemsf\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;postgres\u0026#34; } Connection via port-forward port-forward acid-cluster-1-1 5432:5432 # using psql PGPASSWORD=2rZG1Kx9asdHscswQGzffjdCH3wWr0kguUXUZek6dkemsf psql -h 127.0.0.1 -p 5432 -U postgres # using usql PGPASSWORD=2rZG1Kx9asdHscswQGzffjdCH3wWr0kguUXUZek6dkemsf usql postgresql://postgres@127.0.0.1/postgres Next Steps Congratulations, your first cluster is ready and you were able to connect to it. On the following pages we have put together an introduction with lots of information and details to show you the different possibilities and components of CPO.\n","description":"We can tell and document so much about our project but it seems you just want to get started. Let us show you the fastest way to use CPO.\nPreconditions git helm (optional) kubectl or oc Let\u0026rsquo;s start Step 1 - Preparations To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials Step 2 - Install the Operator Two options are available for the installation:"},{"id":21,"href":"/CYBERTEC-pg-operator/documentation/release_notes/","title":"Release Notes","parent":"Documentation","content":" 0.6.1 Release with fixes\nFixes Backup-Pod now runs with \u0026ldquo;best-effort\u0026rdquo; resource definition Der Init-Container für die Wiederherstellung verwendet nun die gleiche Ressource-Definition wie der Datenbank-Container, wenn es keine spezifische Definition im Cluster-Manifest gibt (spec.backup.pgbackrest.resources) Software-Versions PostgreSQL: 15.3 14.8, 13.11, 12.15 Patroni: 3.0.4 pgBackRest: 2.47 OS: Rocky-Linux 9.1 (4.18) 0.6.0 Release with some improvements and stabilisation measuresm\nFeatures Added Pod Topology Spread Constraints Added support for TDE based on the CYBERTEC PostgreSQL Enterprise Images (Licensed Container Suite) Software-Versions PostgreSQL: 15.3 14.8, 13.11, 12.15 Patroni: 3.0.4 pgBackRest: 2.47 OS: Rocky-Linux 9.1 (4.18) 0.5.0 Release with new Software-Updates and some internal Improvements\nFeatures Updated to Zalando Operator 1.9 Fixes internal Problems with Cronjobs updates for some API-Definitions Software-Versions PostgreSQL: 15.2 14.7, 13.10, 12.14 Patroni: 3.0.2 pgBackRest: 2.45 OS: Rocky-Linux 9.1 (4.18) 0.3.0 Release with some improvements and stabilisation measuresm\nFixes missing pgbackrest_restore configmap fixed Software-Versions PostgreSQL: 15.1 14.7, 13.9, 12.13, 11.18 and 10.23 Patroni: 3.0.1 pgBackRest: 2.44 OS: Rocky-Linux 9.1 (4.18) 0.1.0 Initial Release as a Fork of the Zalando-Operator\nFeatures Added Support for pgBackRest (PoC-State) Stanza-create and Initial-Backup are executed automatically Schedule automatic updates (Full/Incremental/Differential-Backup) Securely store backups on AWS S3 and S3-compatible storage Software-Versions PostgreSQL: 14.6, 13.9, 12.13, 11.18 and 10.23 Patroni: 2.4.1 pgBackRest: 2.42 OS: Rocky-Linux 9.0 (4.18) ","description":"0.6.1 Release with fixes\nFixes Backup-Pod now runs with \u0026ldquo;best-effort\u0026rdquo; resource definition Der Init-Container für die Wiederherstellung verwendet nun die gleiche Ressource-Definition wie der Datenbank-Container, wenn es keine spezifische Definition im Cluster-Manifest gibt (spec.backup.pgbackrest.resources) Software-Versions PostgreSQL: 15.3 14.8, 13.11, 12.15 Patroni: 3.0.4 pgBackRest: 2.47 OS: Rocky-Linux 9.1 (4.18) 0.6.0 Release with some improvements and stabilisation measuresm\nFeatures Added Pod Topology Spread Constraints Added support for TDE based on the CYBERTEC PostgreSQL Enterprise Images (Licensed Container Suite) Software-Versions PostgreSQL: 15."},{"id":22,"href":"/CYBERTEC-pg-operator/level-4/","title":"Roadmap","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":23,"href":"/CYBERTEC-pg-operator/level-3/","title":"Support","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":24,"href":"/CYBERTEC-pg-operator/tags/","title":"Tags","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""}]