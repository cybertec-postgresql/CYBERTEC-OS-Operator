[{"id":0,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/ressources/","title":"Cluster Ressources","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":1,"href":"/CYBERTEC-pg-operator/","title":"CPO (CYBERTEC-PG-Operator)","parent":"","content":"Current Release: 0.7.0 (xx.xx.xxxx) Release Notes\nCPO (CYBERTEC PG Operator) allows you to create and run PostgreSQL clusters on Kubernetes.\nThe operator reduces your efforts and simplifies the administration of your PostgreSQL clusters so that you can concentrate on other things.\nThe following features characterise our operator:\nDeclarative mode of operation Takes over all the necessary steps for setting up and managing the PG cluster. Integrated backup solution, automatic backups and very easy restore (snapshot \u0026amp; PITR) Rolling update procedure for adjustments to the pods and minor updates Major upgrade with minimum interruption time Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing CPO is tested on the following platforms:\nKubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 Rancher AWS EKS Azure AKS Google GKE Furthermore, CPO is basically executable on any CSCF-certified Kubernetes platform.\n","description":"Current Release: 0.7.0 (xx.xx.xxxx) Release Notes\nCPO (CYBERTEC PG Operator) allows you to create and run PostgreSQL clusters on Kubernetes.\nThe operator reduces your efforts and simplifies the administration of your PostgreSQL clusters so that you can concentrate on other things.\nThe following features characterise our operator:\nDeclarative mode of operation Takes over all the necessary steps for setting up and managing the PG cluster. Integrated backup solution, automatic backups and very easy restore (snapshot \u0026amp; PITR) Rolling update procedure for adjustments to the pods and minor updates Major upgrade with minimum interruption time Reduction of downtime thanks to redundancy, pod anti-affinity, auto-failover and self-healing CPO is tested on the following platforms:"},{"id":2,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/backup/","title":"Backup","parent":"Documentation","content":"Backups are essential for databases. From broken storage to deployments gone wrong, backups often save the day. Starting with pg_dump, which was released in the late 1990s, to the archiving of WAL files (PostgreSQL 8.0 / 2005) and pg_basebackup (PostgreSQL 9.0 / 2010), PostgreSQL already offers built-in options for backups and restores based on logical and physical backups.\nBackups with pgBackRest CPO relies on pgBackRest as its backup solution, a tried-and-tested tool with extensive backup and restore options. The backup is based on two elements:\nSnapshots in the form of physical backups WAL archive: Continuous archiving of the WAL files Backups Backups represent a snapshot of the database in the form of pyhsical files. This contains all relevant information that PostgreSQL holds in its data folder. With pgBackRest it is possible to create different types of Backups:\nfull Snapshot: This captures and saves all files at the time of the backup Differential backup: Only captures all files that have been changed since the last full Backup Incremental backup: Only records the files that have been changed since the last backup (of any kind). When restoring using differential or incremental Backup, it is necessary to also use the previous Backup that provide the basis for the selected Backup.\nHINT: The choice of Backup types depends on factors such as the size of the database, the time available for backups and the restore.\nWAL-Archive The WAL (Write-Ahead-Log) refers to log files which record all changes to the database data before they are written to the actual files. The basic idea here is to guarantee the consistency and recoverability of the comitted data even in the event of failures.\nPostgreSQL normally cleans up or recycles the WAL files that are no longer required. By using WAL archiving, the WAL files are saved to a different location before this process so that they can be used for various activities in the future. These activities include\nProviding the WAL files for replicas to keep them up to date Restoring instances that have lost parts of the WAL files in the event of a failure and cannot return to a consistent state without them without losing data Point-In-Time-Recovery (PITR): In contrast to Backups, which map a fixed point in time, WAL files make it possible to jump dynamically to a desired point in time and restore the database to the closest available consistent data point HINT: WAL archiving is an indispensable tool for data availability, recoverability and the continuous availability of PostgreSQL.\nBackup your Cluster With pgBackRest, backups can be stored on different types of storage:\nBlock storage (PVC) S3 / S3-compatible storage Azure blob storage GCS Backups on PVC When using block storage, the operator creates an additional pod that acts as a repo host. Based on a TLS connection, the repo host obtains the data for the Backup from the current primary of the cluster, which is compressed before being sent. WAL archives are pushed from the primary pod to the repo host.\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster namespace: cpo spec: backup: pgbackrest: global: repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container-dev:pgbackrest-16.3-1\u0026#39; repos: - name: repo1 schedule: full: 30 2 * * * storage: pvc volume: size: 15Gi storageClass: default This example creates backups based on a repo host with a daily full Backup at 2:30 am. In addition, pgBackRest is instructed to keep a maximum of 7 full Backups. The oldest one is always removed when a new Backup is created.\nHINT: In addition, further configurations for pgBackRest can be defined in the global object. Information on possible configurations can be found in the pgBackRest documentation\nBackups on S3 pgBackRest can be used directly with AWS S3 or S3-compatible storage such as MinIO, Cloduian HyperStore or SwiftStack.\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster namespace: cpo spec: backup: pgbackrest: configuration: secret: cluster-s3-credential global: repo1-path: /cluster/repo1/ repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container-dev:pgbackrest-16.3-1\u0026#39; repos: - endpoint: \u0026#39;https://s3-zurich.cyberlink.cloud:443\u0026#39; name: repo1 region: zurich resource: cpo-cluster-bucket schedule: full: 30 2 * * * incr: \u0026#39;*/30 * * * *\u0026#39; storage: s3 This example creates a backup in an S3 bucket. In addition to the above configurations, a secret is also required which contains the access data for the S3 storage. The name of the secret must be stored in the spec.backup.pgbackrest.configuration.secret object and the secret must be located in the same namespace as the cluster. Information required to address the S3 bucket:\nEndpoint: S3 api endpoint Region: Region of the bucket resource: Name of the bucket The secret must be defined as follows for the use of S3 storage:\nkind: Secret apiVersion: v1 metadata: name: cluster-s3-credential namespace: cpo stringData: s3.conf | [global] repo1-s3-key=YOUR_S3_KEY repo1-s3-key-secret=YOUR_S3_KEY_SECRET An example with a sercret generator is also available in the tutorials. Enter your access data in the s3.conf file and transfer the tutorial to your Kubernetes with kubectl apply -k cluster-tutorials/pgbackrest_with_s3/.\nEncrypt your backup client-side pgBackRest also allows you to encrypt your backups on the client side before uploading them. This is possible with any type of storage and is very easy to activate.\nFirstly, we need to define an encryption key. This must be specified separately for each repo and stored in the same secret that is defined in the spec.backup.pgbackrest.configuration.secret object.\nkind: Secret apiVersion: v1 metadata: name: cluster-s3-credential namespace: cpo stringData: s3.conf | [global] repo1-s3-key=YOUR_S3_KEY repo1-s3-key-secret=YOUR_S3_KEY_SECRET repo1-cipher-pass=YOUR_ENCRYPTION_KEY We also need to configure the type of encryption for pgBackRest. This is done via the cipher-type parameter, which must also be specified for each repo. You can find the available values for the parameter here\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster namespace: cpo spec: backup: pgbackrest: configuration: secret: cluster-s3-credential global: repo1-path: /cluster/repo1/ repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count repo1-cipher-type: aes-256-cbc image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container-dev:pgbackrest-16.3-1\u0026#39; repos: - endpoint: \u0026#39;https://s3-zurich.cyberlink.cloud:443\u0026#39; name: repo1 region: zurich resource: cpo-cluster-bucket schedule: full: 30 2 * * * incr: \u0026#39;*/30 * * * *\u0026#39; storage: s3 How a Backup works The operator creates a cronjob object on Kubernetes based on the defined times for automatic backups. This means that the Kubernetes core (CronJob Controller) will take care of processing the automatic backups and create a job and thus a pod at the appropriate time. The pod will send the backup command to the primary or, if block storage is used, to the repo host and monitor it. As soon as the backup is successfully completed, the pod stops with Completed and thus completes the job.\nkubectl get cronjobs --------------------------------------------------------------------------------------- NAME | SCHEDULE | SUSPEND | ACTIVE | LAST SCHEDULE | AGE pgbackrest-cluster-repo1-full | 30 2 * * * | False | 0 | 4h46m | 14h pgbackrest-cluster-repo1-incr | */30 * * * * | False | 1 | 81s | 106m kubectl get jobs ----------------------------------------------------------------------- NAME | COMPLETIONS | DURATION | AGE pgbackrest-cluster-repo1-full-28597110 | 1/1 | 52s | 140m pgbackrest-cluster-repo1-incr-28597365 | 1/1 | 2m37s | 32m pgbackrest-cluster-repo1-incr-28597380 | 1/1 | 2m38s | 17m pgbackrest-cluster-repo1-incr-28597395 | 0/1 | 2m3s | 2m3s If there are problems such as a timeout, the pod will stop with exit code 1 and thus indicate an error. In this case, a new pod will be created which will attempt to complete the backup. The maximum number of attempts is 6, so if the backup fails six times, the job is deemed to have failed and will not be attempted again until the next cronjob execution. The job pod log provides information about the problems.\nkubectl get pods ----------------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-0 | 2/2 | Running | 2 | 14h cluster-pgbackrest-repo-host-0 | 1/1 | Running | 0 | 107m pgbackrest-cluster-repo1-full-28597110-x8zpw | 0/1 | Completed | 0 | 143m pgbackrest-cluster-repo1-incr-28597365-7bb5l | 0/1 | Completed | 0 | 34m pgbackrest-cluster-repo1-incr-28597380-j76rr | 0/1 | Completed | 0 | 19m pgbackrest-cluster-repo1-incr-28597395-rh86t | 0/1 | Completed | 0 | 4m27s postgres-operator-66bbff5c54-5sjmk | 1/1 | Running | 0 | 47m ","description":"Backups are essential for databases. From broken storage to deployments gone wrong, backups often save the day. Starting with pg_dump, which was released in the late 1990s, to the archiving of WAL files (PostgreSQL 8.0 / 2005) and pg_basebackup (PostgreSQL 9.0 / 2010), PostgreSQL already offers built-in options for backups and restores based on logical and physical backups.\nBackups with pgBackRest CPO relies on pgBackRest as its backup solution, a tried-and-tested tool with extensive backup and restore options."},{"id":3,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/ha-cluster/","title":"High Availability Cluster (HA)","parent":"Documentation","content":"To ensure continiues productive usage you can create a HA-Cluster or modify a Single-Node-Cluster to a HA-Cluster. The needed changes are less complicated\nspec: numberOfInstances: 2 # or more The example above will create a HA-Cluster based on two Nodes.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 54s cluster-1-1 | 1/1 | Running | 0 | 31s ","description":"To ensure continiues productive usage you can create a HA-Cluster or modify a Single-Node-Cluster to a HA-Cluster. The needed changes are less complicated\nspec: numberOfInstances: 2 # or more The example above will create a HA-Cluster based on two Nodes.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 54s cluster-1-1 | 1/1 | Running | 0 | 31s "},{"id":4,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/modify-cluster/","title":"Modify Cluster","parent":"Documentation","content":"Starting with the Single-Node-Cluster from the previous section, we want to modify the Instance a bit to see.\nCPU and Memory spec: resources: limits: cpu: 1000m memory: 500Mi requests: cpu: 500m memory: 500mi Based on the ressources-Definiton we\u0026rsquo;re able to modify the reserved Hardware (requests) and the limits, which allows use to consume more than the reserved definitons if the k8s-worker has this hardware available. There are some Restrictions when modifiying the limits-section. Because of the behaviour of Databases we should never define a diff between requests.memory and limits.memory. A Database is after some time using all available Memory, for Cache and other things. Limits are optional and the worker node can force them back. forcing back memory will create big problems inside a database like creating corruption, forcing OutOfMemory-Killer and so on. CPU on the other side is a ressource we can use inside the limits definiton to allow our database using more cpu if needed and available.\nSidecars Sidecars are further Containers running on the same Pod as the Database. We can use them for serveral different Jobs. The Operator allows us to define them directly inside the Cluster-Manifest.\nspec: sidecars: - name: \u0026#34;telegraf-sidecar\u0026#34; image: \u0026#34;telegraf:latest\u0026#34; ports: - name: metrics containerPort: 8094 protocol: TCP resources: limits: cpu: 500m memory: 500Mi requests: cpu: 100m memory: 100Mi env: - name: \u0026#34;USEFUL_VAR\u0026#34; value: \u0026#34;perhaps-true\u0026#34; This Example will add a second Container to our Pods. This will trigger a restart, which creates Downtime if you\u0026rsquo;re not running a HA-Cluster.\nInit-Containers We can exactly the same as for sidecars also for Init-Containers. The difference is, that a sidecar is running normally on a pod. An Init-Container will just run as first container when the pod is created and it will ends after his job is done. The \u0026ldquo;normal\u0026rdquo; Containers has to wait till all init-Containers finished their jobs and ended with a exit-status.\nspec: initContainers: - name: date image: busybox command: [ \u0026#34;/bin/date\u0026#34; ] TLS-Certificates One Startup the Containers will create a custom TLS-Certificate which allows creating tls-secured-connections to the Database. But this Certificates cannot verified, because the application has no information about the CA. Because of this the certificates are no protection against MITM-Attacks. You\u0026rsquo;re able to configure your own Certificates and CA to ensure, that you can use secured and verified connections between your application and your database.\nspec: tls: secretName: \u0026#34;\u0026#34; # should correspond to a Kubernetes Secret resource to load certificateFile: \u0026#34;tls.crt\u0026#34; privateKeyFile: \u0026#34;tls.key\u0026#34; caFile: \u0026#34;\u0026#34; # optionally configure Postgres with a CA certificate caSecretName: \u0026#34;\u0026#34; # optionally the ca.crt can come from this secret instead. You need to store the needed values from tls.crt, tls.key and ca.crt in a secret and define the secrtetname inside the tls-object. if you want you can create a separate sercet just for the ca and use this secret for every cluster inside the Namespace. To get Information about creating Certificates and the secrets check the Tutorial in the additonal-Section or click here\nNode-Affinity Node-Affinity will ensure that the Cluster-pods only deployed on Kubernetes-Nodes which has the defined Labelkey and -Value\nspec: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: cpo operator: In values: - enabled This allowes you to use specific database-nodes in a mixed cluster for example. In the Example above the Cluster-Pods are just deployed on Nodes with the Key: cpo and the value: enabled So you\u0026rsquo;re able to seperate your Workload.\nPostgreSQL-Configuration Every Cluster will start with the default PostgreSQL-Configuration. Every Parameter can be overriden based in definitions inside the Cluster-Manifest. Therefore we just need a add the section parameters to the postgresql-Object\nspec: postgresql: version: 16 parameters: max_connections: \u0026#34;53\u0026#34; log_statement: \u0026#34;all\u0026#34; track_io_timing: \u0026#34;true\u0026#34; These Definitions will change the PostgreSQL-Configuration. Based on the needs of Parameter changes the Pods may needs a restart, which creates a Downtime if its not a HA-Cluster. You can check Parameters and allowed Values on this Sources to ensure a correct Value.\nPostgreSQL Documentation PostgreSQL.org PostgreSQLco.nf ","description":"Starting with the Single-Node-Cluster from the previous section, we want to modify the Instance a bit to see.\nCPU and Memory spec: resources: limits: cpu: 1000m memory: 500Mi requests: cpu: 500m memory: 500mi Based on the ressources-Definiton we\u0026rsquo;re able to modify the reserved Hardware (requests) and the limits, which allows use to consume more than the reserved definitons if the k8s-worker has this hardware available. There are some Restrictions when modifiying the limits-section."},{"id":5,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/restore/","title":"Restore","parent":"Documentation","content":"Restore or recovery is the process of starting a PostgreSQL instance or a cluster based on a defined and existing backup. This can be just a Backup or a combination of a Backup and additional WAL files. The difference is that a Backup represents a fixed point in time, whereas the combination with WAL enables a point-in-time recovery(PITR).\nYou can find more information about backups here\nRescue my cluster CPO enables the restore based on an existing backup using pgBackRest. To do this, it needs the relevant information about the point in time or snapBackupshot to which it should restore and where the data for this comes from. As we have already provided the operator with all the information relating to the storage of backups in the previous chapter, it only needs the following information:\nid: Control variable, must be increased for each restore process type: What type of restore is required repo: Which repo the data should come from set: Specific Backup to restore HINT: To ensure that the operator does not repeat an already done restore, the defined object id in the restore section is saved by the operator, so the value of this id must be changed for a new restore.\nDetails for a Backup restore With this information, we define a fixed Backup from repo1 and that pgBackRest should stop at the end of the Backup\nrestore: id: \u0026#39;1\u0026#39; options: - \u0026#39;--type=immediate\u0026#39; - \u0026#39;--set=20240515-164100F\u0026#39; repo: \u0026#39;1\u0026#39; HINT: Without the specification --type=immediate, pgBackRest would then consume the entire WAL that is available and thus restore the last available consistent data point.\nDetails for a point-in-time recoery (PITR) We use this information to define a point-in-time recovery (PITR) and define the end point using a timestamp and the start point using a Backup specification. The latter is optional. Without this information, pgBackRest would automatically start at the last previous full Backup.\nrestore: id: \u0026#39;1\u0026#39; options: - \u0026#39;--type=time\u0026#39; - \u0026#39;--set=20240515-164100F\u0026#39; - \u0026#39;--target=2024-05-16 07:46:05.506817+00\u0026#39; repo: \u0026#39;1\u0026#39; HINT: --type=time indicates that it is a point-in-time recovery (PITR).\nExample in a cluster manifest apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-5 namespace: cpo spec: backup: pgbackrest: configuration: secret: cluster-pvc-credentials global: repo1-retention-full: \u0026#39;7\u0026#39; repo1-retention-full-type: count image: \u0026#39;docker.io/cybertecpostgresql/cybertec-pg-container-dev:pgbackrest-16.2-8-dev\u0026#39; repos: - name: repo1 schedule: full: 30 2 * * * storage: pvc volume: size: 1Gi restore: id: \u0026#39;1\u0026#39; options: - \u0026#39;--type=time\u0026#39; - \u0026#39;--set=20240515-164100F\u0026#39; - \u0026#39;--target=2024-05-16 07:46:05.506817+00\u0026#39; An example of this can also be found in our tutorials. For a point-in-time recovery (PITR) you can find it here.\nATTENTION: Incorrect information for the Backup or the timestamp can result in pgBackRest not being able to complete the restore successfully. In the event of an error, the information must be corrected and another restore must be started.\n","description":"Restore or recovery is the process of starting a PostgreSQL instance or a cluster based on a defined and existing backup. This can be just a Backup or a combination of a Backup and additional WAL files. The difference is that a Backup represents a fixed point in time, whereas the combination with WAL enables a point-in-time recovery(PITR).\nYou can find more information about backups here\nRescue my cluster CPO enables the restore based on an existing backup using pgBackRest."},{"id":6,"href":"/CYBERTEC-pg-operator/documentation/examples/","title":"Single Cluster","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":7,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/clone/","title":"Single Cluster","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":8,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/ConnectionPooler/","title":"Single Cluster","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":9,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/databases_roles/","title":"Single Cluster","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":10,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/examples/","title":"Single Cluster","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":11,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/sidecars/","title":"Single Cluster","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":12,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/single-cluster/","title":"Single Cluster","parent":"Documentation","content":"To set up a cluster, the implementation is based on a description, as with the other Kubernetes deplyoments. To do this, the operator uses a document of type postgresql.\nYou can also find the basic minimum specifications for a single-node cluster in our tutorial project on Github\nminimal Single-Node Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s HINT: Here you will find a complete overview of the available options within the cluster manifest.\nminimal High-Availability Cluster No more effort is required to create a High-Availablity cluster than for a Single-Node Cluster. Only the Cluster-Manifest needs to be modified slightly. The difference lies in the object numberOfInstances, which must be set \u0026gt; 1.\nYou can also find the basic minimum specifications for a High-Availability-Cluster cluster in our tutorial project on Github\napiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 2 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi You can either create a new cluster with the document or update an existing cluster with it. This makes it possible to scale the cluster up and down during operation.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 2m12s cluster-1-1 | 1/1 | Running | 0 | 50s ","description":"To set up a cluster, the implementation is based on a description, as with the other Kubernetes deplyoments. To do this, the operator uses a document of type postgresql.\nYou can also find the basic minimum specifications for a single-node cluster in our tutorial project on Github\nminimal Single-Node Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":13,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/slots/","title":"Single Cluster","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":14,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/standby/","title":"Single Cluster","parent":"Documentation","content":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server. Also created is a volume based on your default-storage Class. The Ressource-Definiton means, that we reserve a half cpu and a half GB Memory for this Cluster with the same Definition as limit.\nAfter some seconds we should see, that the operator creates our cluster based on the declared definitions.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 1/1 | Running | 0 | 50s We can now starting to modify our cluster with some more Definitons.\nUse a specific Storageclass spec: ... volume: size: 5Gi storageClass: default-provisioner ... Using the storageClass-Definiton allows us to define a specific storageClass for this Cluster. Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nExpanding Volume The Operator allows to you expand your volume if the storage-System is able to do this.\nspec: ... volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Setting up a basic Cluster is pretty easy, we just need the minimum Definiton of a cluster-manifest which can also be find in the operator-tutorials repo on github. We need the following Definitions for the basic cluster.\nminimal Single Cluster apiVersion: cpo.opensource.cybertec.at/v1 kind: postgresql metadata: name: cluster-1 spec: dockerImage: \u0026#34;docker.io/cybertecpostgresql/cybertec-pg-container:postgres-16.1-6-dev\u0026#34; numberOfInstances: 1 postgresql: version: \u0026#34;16\u0026#34; resources: limits: cpu: 500m memory: 500Mi requests: cpu: 500m memory: 500Mi volume: size: 5Gi Based on this Manifest the Operator will deploy a single-Node-Cluster based on the defined dockerImage and start the included Postgres-16-Server."},{"id":15,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/monitoring/","title":"Start with Monitoring","parent":"Documentation","content":"The CPO-Project has prepared severall Tools which allows to setup a Monitoring-Stack including Alerting and Metric-Viewer. These Stack is based on:\nPrometheus Alertmanager Grafana exporter-container CPO has prepared an own Exporter for the PostgreSQl-Pod which can used as a sidecar.\nSetting up the Monitoring Stack To setup the Monitoring-Stack we suggest that you create an own namespace and use the prepared kustomization file inside the Operator-Tutorials.\n$ kubectl create namespace cpo-monitoring namespace/cpo-monitoring created $ kubectl get pods -n cpo-monitoring No resources found in cpo-monitoring namespace. git clone https://github.com/cybertec-postgresql/CYBERTEC-operator-tutorial cd CYBERTEC-operator-tutorial/setup/monitoring # Hint: Please check if youn want to use a specific storage-class the file pvcs.yaml and add your storageclass on the commented part. Please ensure that you removed the comment-char. $ kubectl apply -n cpo-monitoring -k . serviceaccount/cpo-monitoring created serviceaccount/cpo-monitoring-tools created clusterrole.rbac.authorization.k8s.io/cpo-monitoring unchanged clusterrolebinding.rbac.authorization.k8s.io/cpo-monitoring unchanged configmap/alertmanager-config created configmap/alertmanager-rules-config created configmap/cpo-prometheus-cm created configmap/grafana-dashboards created configmap/grafana-datasources created secret/grafana-secret created service/cpo-monitoring-alertmanager created service/cpo-monitoring-grafana created service/cpo-monitoring-prometheus created persistentvolumeclaim/alertmanager-pvc created persistentvolumeclaim/grafana-pvc created persistentvolumeclaim/prometheus-pvc created deployment.apps/cpo-monitoring-alertmanager created deployment.apps/cpo-monitoring-grafana created deployment.apps/cpo-monitoring-prometheus created Hint: If you\u0026#39;re not running Openshift you will get a error like this: error: resource mapping not found for name: \u0026#34;grafana\u0026#34; namespace: \u0026#34;\u0026#34; from \u0026#34;.\u0026#34;: no matches for kind \u0026#34;Route\u0026#34; in version \u0026#34;route.openshift.io/v1\u0026#34; ensure CRDs are installed first You can ignore this, because it depends on an object with the type route which is part of Openshift. It is not needed replaced by ingress-rules or an loadbalancer-service. After installing the Monitoring-Stack we\u0026rsquo;re able to check the created pods inside the namespace\n$ kubectl get pods -n cpo-monitoring ---------------------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cpo-monitoring-alertmanager-5bb8bc79f7-8pdv4 | 1/1 | Running | 0 | 3m35s cpo-monitoring-grafana-7c7c4f787b-jbj2f | 1/1 | Running | 0 | 3m35s cpo-monitoring-prometheus-67969b757f-k26jd | 1/1 | Running | 0 | 3m35s The configuration of this monitoring-stack is based on severall configmaps which can be modified.\nPrometheus-Configuration Alertmanager-Configuration Grafana-Configuration Configure a PostgreSQL-Cluster to allow Prometheus to gather metrics To allow Prometheus to gather metrics from your cluster you need to do some small modfications on the Cluster-Manifest. We need to create the monitor-object for this:\nkubectl edit postgresqls.cpo.opensource.cybertec.at cluster-1 ... spec: ... monitor: image: docker.io/cybertecpostgresql/cybertec-pg-container:exporter-16.2-1 The Operator will add automatically the monitoring sidecar to your pods, create a new postgres-user and add some structure inside the postgres-database to enable everthing needed for the Monitoring. Also every Ressource of your Cluster will get a new label: cpo_monitoring_stack=true. This is needed for Prometheus to identify all clusters which should be added to the monitoring. Removing this label will stop Prometheus to gather data from this cluster.\nAfter changing your Cluster-Manifest the Pods needs to be recreated which is done by a rolling update. After this you can see that the pod has now more than just one container.\nkubectl get pods ----------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cluster-1-0 | 2/2 | Running | 0 | 54s cluster-1-1 | 2/2 | Running | 0 | 31s You can check the logs to see that the exporter is working and with curl you can see the output of the exporter.\nkubectl logs cluster-1-0 -c postgres-exporter kubectl exec --stdin --tty cluster-1-0 -c postgres-exporter -- /bin/bash [exporter@cluster-1-0 /]# curl http://127.0.0.1:9187/metrics You can now setup a LoadBalancer-Service or create an Ingress-Rule to allow access von outside to the grafana. Alternativ you can use a port-forward.\nLoadBalancer or Nodeport Ingress-Rule Port-Forwarding $ kubectl get pods -n cpo-monitoring ---------------------------------------------------------------------------------------- NAME | READY | STATUS | RESTARTS | AGE cpo-monitoring-alertmanager-5bb8bc79f7-8pdv4 | 1/1 | Running | 0 | 6m42s cpo-monitoring-grafana-7c7c4f787b-jbj2f | 1/1 | Running | 0 | 6m42s cpo-monitoring-prometheus-67969b757f-k26jd | 1/1 | Running | 0 | 6m42s $ kubectl port-forward cpo-monitoring-grafana-7c7c4f787b-jbj2f -n cpo-monitoring 9000:9000 Forwarding from 127.0.0.1:9000 -\u0026gt; 9000 Forwarding from [::1]:9000 -\u0026gt; 9000 Call http://localhost:9000 in the Browser\nUse a Route (Openshift only) kubectl get route -n cpo-monitoring Use the Route-Adress to access Grafana\n","description":"The CPO-Project has prepared severall Tools which allows to setup a Monitoring-Stack including Alerting and Metric-Viewer. These Stack is based on:\nPrometheus Alertmanager Grafana exporter-container CPO has prepared an own Exporter for the PostgreSQl-Pod which can used as a sidecar.\nSetting up the Monitoring Stack To setup the Monitoring-Stack we suggest that you create an own namespace and use the prepared kustomization file inside the Operator-Tutorials.\n$ kubectl create namespace cpo-monitoring namespace/cpo-monitoring created $ kubectl get pods -n cpo-monitoring No resources found in cpo-monitoring namespace."},{"id":16,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/storage/","title":"Storage","parent":"Documentation","content":"Storage is crucial for the performance of a database and is therefore a central element. As with systems based on bare metal or virtual machines, the same requirements apply to Kubernetes workloads, such as constant availability, good performance, consistency and durability.\nA basic distinction is made between local storage, which is directly connected to the worker node, and network storage, which is mounted on the worker node and thus made available to the pod.\nIn probably the vast majority of Kubernetes systems, network storage is used, for example from systems from hyperscalers or other cloud providers or own systems such as CEPH.\nWith network storage in particular, attention must be paid to performance in terms of throughput (speed and guaranteed IOPS) and, above all, latency. It is also important to ensure that the different volumes do not compete with each other in terms of load.\nPAY ATTENTION: Before using a CPO cluster, make sure that the storage is suitable for the intended use and provides the necessary performance. In addition, check the storage with benchmarks before use. We recommend the use of pgbench for this purpose.\nDefine Storage-Volume The storage is defined via the volume object and enables the size and storage class for the storage to be defined, among other things.\nspec: volume: size: 5Gi storageClass: default-provisioner ... The volume is currently used for both PG and WAL data. In future, there will be an optional option to create a separate WAL volume. Please check our roadmap\nPAY ATTENTION: Please ensure, that the storageClass exists and is usable. If a Volume cannot provide the Volume will stand in the pending-State as like the Database-Pod.\nThe volume is currently used for both PG and WAL data. In future, there will be an optional option to create a separate WAL volume.\nExpanding Volume HINT: Kubernetes is able to forward requests to expand the storage to the storage system and enable the expand without the need to restart the container. However, this also requires the associated storage system and the driver used to support this. This information can be found in the storage class under the object: allowVolumeExpansion. A distinction must also be made between online and offline expand. The latter requires a restart of the pod. To do this, the pod must be deleted manually.\nTo Expand the Volume, the value of the object volume.size must be increased\nspec: volume: size: 10Gi storageClass: default-provisioner ... This will trigger the expand of your Cluster-Volumes. It will need some time and you can check the current state inside the pvc.\nkubectl get pvc pgdata-cluster-1-0 -o yaml ------------------------------------------------------- spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: crc-csi-hostpath-provisioner volumeMode: Filesystem volumeName: pvc-800d7ecc-2d5f-4ef4-af83-1cd94c766d37 status: accessModes: - ReadWriteOnce capacity: storage: 5Gi phase: Bound Creating additonal Volumes The Operator allows you to modify your cluster with additonal Volumes.\nspec: ... additionalVolumes: - name: empty mountPath: /opt/empty targetContainers: - all volumeSource: emptyDir: {} This example will create an emptyDir and mount it to all Containers inside the Database-Pod.\nSpecific Settings for aws gp3 Storage For the gp3 Storage aws you can define more informations\nvolume: size: 1Gi storageClass: gp3 iops: 1000 # for EBS gp3 throughput: 250 # in MB/s for EBS gp3 The defined IOPS and Throughput will include in the PersistentVolumeClaim and send to the storage-Provisioner. Please keep in Mind, that on aws there is a CoolDown-Time as a limitation defined. For new Changes you need to wait 6 hours. Please also ensure to check the default and allowed values for IOPS and Throughput AWS docs.\nTo ensure that the settings are updates properly please define the Operator-Configuration \u0026lsquo;storage_resize_mode\u0026rsquo; from default to \u0026lsquo;mixed\u0026rsquo;\n","description":"Storage is crucial for the performance of a database and is therefore a central element. As with systems based on bare metal or virtual machines, the same requirements apply to Kubernetes workloads, such as constant availability, good performance, consistency and durability.\nA basic distinction is made between local storage, which is directly connected to the worker node, and network storage, which is mounted on the worker node and thus made available to the pod."},{"id":17,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/configuration/","title":"Configuration","parent":"Documentation","content":"","description":""},{"id":18,"href":"/CYBERTEC-pg-operator/documentation/crd/crd-postgresql/","title":"CRD: postgresql","parent":"Documentation","content":" postgresql Name Type required Description apiVersion string true acid.zalando.do/v1 kind string true metadata object true spec object true status object false spec Name Type required Description backup object false Enables the definition of a customised backup solution for the cluster teamId string true name of the team the cluster belongs to numberOfInstances Int true Number of nodes of the cluster dockerImages string false Define a custom image to override the default schedulerName string false Define a custom Name to override the default spiloRunAsUser string false Define an User id which should be used for the pods spiloRunAsGroup string false Define an Group id which should be used for the pods spiloFSGroup string false Persistent Volumes for the pods in the StatefulSet will be owned and writable by the group ID specified. enableMasterLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres primary enableMasterPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the primary ConnectionPooler enableReplicaLoadBalancer boolean false Define whether to enable the load balancer pointing to the Postgres replicas enableReplicaPoolerLoadBalancer boolean false Define whether to enable the load balancer pointing to the Replica-ConnectionPooler allowedSourceRange string false Defines the range of IP networks (in CIDR-notation). The corresponding load balancer is accessible only to the networks defined by this parameter. users map false a map of usernames to user flags for the users that should be created in the cluster by the operator usersWithSecretRotation list false list of users to enable credential rotation in K8s secrets. The rotation interval can only be configured globally. usersWithInPlaceSecretRotation list false list of users to enable in-place password rotation in K8s secrets. The rotation interval can only be configured globally. databases map false a map of databases that should be created in the cluster by the operator tolerations list false a list of tolerations that apply to the cluster pods. Each element of that list is a dictionary with the following fields: key, operator, value, effect and tolerationSeconds podPriorityClassName string false a name of the priority class that should be assigned to the cluster pods. If not set then the default priority class is taken. The priority class itself must be defined in advance podAnnotations map false A map of key value pairs that gets attached as annotations to each pod created for the database. ServiceAnnotations map false A map of key value pairs that gets attached as annotations to each Service created for the database. enableShmVolume boolean false Start a database pod without limitations on shm memory. By default Docker limit /dev/shm to 64M (see e.g. the docker issue, which could be not enough if PostgreSQL uses parallel workers heavily. If this option is present and value is true, to the target database pod will be mounted a new tmpfs volume to remove this limitation. enableConnectionPooler boolean false creates a ConnectionPooler for the primary Database enableReplicaConnectionPooler boolean false creates a ConnectionPooler for the replica Databases enableLogicalBackup boolean false Enable logical Backups for this Cluster (Stored on S3) - s3-configuration for Operator is needed (Not for pgBackRest) logicalBackupSchedule string false Schedule for the logical backup K8s cron job. (Not for pgBackRest) additionalVolumes list false List of additional volumes to mount in each container of the statefulset pod. Each item must contain a name, mountPath, and volumeSource which is a kubernetes volumeSource. It allows you to mount existing PersistentVolumeClaims, ConfigMaps and Secrets inside the StatefulSet. back to parent\nbackup Name Type required Description pgbackrest object false Enables the definition of a pgbackrest-setup for the cluster back to parent\npgbackrest Name Type required Description configuration object false Enables the definition of a pgbackrest-setup for the cluster global object false images string true repos array true resources: object false Resource definition (limits.cpu, limits.memory \u0026amp; requests.cpu \u0026amp; requests.memory) back to parent\nconfiguration Name Type required Description secret object false Secretname with the contained S3 credentials (AccessKey \u0026amp; SecretAccessKey) (Note: must be placed in the same namespace as the cluster) protection object false Enable Protection-Options back to parent\nprotection Name Type required Description restore boolean false A restore is ignored as long as this option is set to true. back to parent\nrepos Name Type required Description name string true Name of the repository Required:Repo[1-4] storage string true Defines the used backup-storage (Choose from List: local,s3,blob,gcs) Currently s3 only! resource string true Bucket-/Instance-/Storage- or PVC-Name endpoint string false The Endpoint for the choosen Storage (Not required for local storage) region string false Region for the choosen Storage (S3 only) schedule string false Object for defining automatic backups back to parent\nschedule Name Type required Description full string false (Cron-Syntax) Define full backup incr string false (Cron-Syntax) Define incremental backup diff string false (Cron-Syntax) Define differential backup back to parent\nstatus Name Type required Description PostgresClusterStatus string false Shows the cluster status. Filled by the Operator back to parent\n","description":"postgresql Name Type required Description apiVersion string true acid.zalando.do/v1 kind string true metadata object true spec object true status object false spec Name Type required Description backup object false Enables the definition of a customised backup solution for the cluster teamId string true name of the team the cluster belongs to numberOfInstances Int true Number of nodes of the cluster dockerImages string false Define a custom image to override the default schedulerName string false Define a custom Name to override the default spiloRunAsUser string false Define an User id which should be used for the pods spiloRunAsGroup string false Define an Group id which should be used for the pods spiloFSGroup string false Persistent Volumes for the pods in the StatefulSet will be owned and writable by the group ID specified."},{"id":19,"href":"/CYBERTEC-pg-operator/documentation/","title":"Documentation","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":20,"href":"/CYBERTEC-pg-operator/documentation/crd/","title":"Documentation","parent":"Documentation","content":"","description":""},{"id":21,"href":"/CYBERTEC-pg-operator/documentation/how-to-use/installation/","title":"Installation","parent":"Documentation","content":" Prerequisites For the installation you either need our CPO tutorial repository or you install CPO directly from our registry.\nException: Installation via Operatorhub (Openshift only)\nCPO-Tutorial-Repository To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials CPO-Registry Create Namespace # kubectl kubectl create namespace cpo # oc oc create namespace cpo Install CPO There are several ways to install CPO:\nUse Helm Use apply Use Operatorhub (On Openshift only) Helm You can check and change the value.yaml of the helm diagram under the path helm/operator/values.yaml By default, the operator is defined so that it is configured via crd-configuration. If you wish, you can change this to configmap. There are also some other default settings.\nhelm install -n cpo cpo helm/operator/. The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\nApply The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\nOperatorhub The installation uses a standard configuration. On the following page you will find more information on how to configure cpo and thus adapt it to your requirements.\n","description":"Prerequisites For the installation you either need our CPO tutorial repository or you install CPO directly from our registry.\nException: Installation via Operatorhub (Openshift only)\nCPO-Tutorial-Repository To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials CPO-Registry Create Namespace # kubectl kubectl create namespace cpo # oc oc create namespace cpo Install CPO There are several ways to install CPO:"},{"id":22,"href":"/CYBERTEC-pg-operator/documentation/crd/crd-operator-configurator/","title":"Operator-Configuration","parent":"Documentation","content":" Name Type default Description enable_crd_registration boolean true crd_categories string all enable_lazy_spilo_upgrade boolean false enable_pgversion_env_var boolean true enable_spilo_wal_path_combat boolean false etcd_host string kubernetes_use_configmaps boolean false docker_image string sidecars list enable_shm_volume boolean true workers int 8 max_instances int -1 min_instances int -1 resync_period string 30m repair_period string 5m set_memory_request_to_limit boolean false debug_logging boolean true enable_db_access boolean true spilo_privileged boolean false spilo_allow_privilege_escalation boolean true watched_namespace string * major-upgrade-specific Name Type default Description major_version_upgrade_mode string off major_version_upgrade_team_allow_list string minimal_major_version string 9.6 target_major_version string 14 aws-specific Name Type default Description wal_s3_bucket string log_s3_bucket string kube_iam_role string aws_region string additional_secret_mount string additional_secret_mount_path string enable_ebs_gp3_migration boolean enable_ebs_gp3_migration_max_size int logical-backup-specific Name Type default Description logical_backup_docker_image string logical_backup_google_application_credentials string logical_backup_job_prefix string logical_backup_provider string logical_backup_s3_access_key_id string logical_backup_s3_bucket string logical_backup_s3_endpoint string logical_backup_s3_region string logical_backup_s3_secret_access_key string logical_backup_s3_sse string logical_backup_s3_retention_time string logical_backup_schedule string (Cron-Syntax) team-api-specific Name Type default Description enable_teams_api string teams_api_url string teams_api_role_configuration string enable_team_superuser boolean team_admin_role boolean enable_admin_role_for_users boolean pam_role_name string pam_configuration string protected_role_names list postgres_superuser_teams string role_deletion_suffix string enable_team_member_deprecation boolean enable_postgres_team_crd boolean enable_postgres_team_crd_superusers boolean enable_team_id_clustername_prefix boolean ","description":"Name Type default Description enable_crd_registration boolean true crd_categories string all enable_lazy_spilo_upgrade boolean false enable_pgversion_env_var boolean true enable_spilo_wal_path_combat boolean false etcd_host string kubernetes_use_configmaps boolean false docker_image string sidecars list enable_shm_volume boolean true workers int 8 max_instances int -1 min_instances int -1 resync_period string 30m repair_period string 5m set_memory_request_to_limit boolean false debug_logging boolean true enable_db_access boolean true spilo_privileged boolean false spilo_allow_privilege_escalation boolean true watched_namespace string * major-upgrade-specific Name Type default Description major_version_upgrade_mode string off major_version_upgrade_team_allow_list string minimal_major_version string 9."},{"id":23,"href":"/CYBERTEC-pg-operator/documentation/how_to_use/","title":"Overview","parent":"Documentation","content":"","description":""},{"id":24,"href":"/CYBERTEC-pg-operator/documentation/level-2-3/","title":"Overview","parent":"Documentation","content":"","description":""},{"id":25,"href":"/CYBERTEC-pg-operator/level-1-1-overview/","title":"Overview","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":26,"href":"/CYBERTEC-pg-operator/level-1/","title":"Overview","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":27,"href":"/CYBERTEC-pg-operator/news/","title":"Overview","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":28,"href":"/CYBERTEC-pg-operator/overview/","title":"Overview","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":29,"href":"/CYBERTEC-pg-operator/documentation/quickstart/","title":"Quickstart","parent":"Documentation","content":"We can tell and document so much about our project but it seems you just want to get started. Let us show you the fastest way to use CPO.\nPreconditions git helm (optional) kubectl or oc Let\u0026rsquo;s start Step 1 - Preparations To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials Step 2 - Install the Operator Two options are available for the installation:\nInstallation via Helm-Chart Installation via apply Installation via Helm-Chart kubectl apply -k setup/namespace/. helm install cpo setup/helm/operator/ -n cpo Installation via apply kubectl apply -k setup/namespace/. kubectl apply -k setup/helm/operator/. -n cpo You can check if the operator pod is in operation.\nkubectl get pods -n cpo --selector=cpo.cybertec.at/pod/type=postgres-operator The result should look like this:\nNAME READY STATUS RESTARTS AGE postgres-operator-599688d948-fw8pw 1/1 Running 0 41s The operator is ready and the setup is complete. The next step is the creation of a Postgres cluster\nStep 3 - Create a Cluster To create a simple cluster, the following command is sufficient\nkubectl apply -k cluster-tutorials/single-cluster watch kubectl get pods --selector cluster-name=cluster-1,application=spilo The result should look like this:\nAlle 2.0s: kubectl get pods --selector cluster-name=cluster-1,application=spilo NAME READY STATUS RESTARTS AGE cluster-1-0 2/2 Running 0 28s cluster-1-1 0/2 PodInitializing 0 9s Step 4 - Connect to the Database Get your login information from the secret.\nkubectl get secret postgres.acid-cluster-1.credentials.postgresql.acid.zalan.do -o jsonpath=\u0026#39;{.data}\u0026#39; | jq \u0026#39;.|map_values(@base64d)\u0026#39; The result should look like this:\n{ \u0026#34;password\u0026#34;: \u0026#34;2rZG1Kx9asdHscswQGzff4Ru0xW6uasacy3GQ0sjdCH3wWr0kguUXUZek6dkemsf\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;postgres\u0026#34; } Connection via port-forward port-forward acid-cluster-1-1 5432:5432 # using psql PGPASSWORD=2rZG1Kx9asdHscswQGzffjdCH3wWr0kguUXUZek6dkemsf psql -h 127.0.0.1 -p 5432 -U postgres # using usql PGPASSWORD=2rZG1Kx9asdHscswQGzffjdCH3wWr0kguUXUZek6dkemsf usql postgresql://postgres@127.0.0.1/postgres Next Steps Congratulations, your first cluster is ready and you were able to connect to it. On the following pages we have put together an introduction with lots of information and details to show you the different possibilities and components of CPO.\n","description":"We can tell and document so much about our project but it seems you just want to get started. Let us show you the fastest way to use CPO.\nPreconditions git helm (optional) kubectl or oc Let\u0026rsquo;s start Step 1 - Preparations To get started, you can fork our tutorial repository on Github and then download it. CYBERTEC-operator-tutorials\nGITHUB_USER=\u0026#39;[YOUR_USERNAME]\u0026#39; git clone https://github.com/$GITHUB_USER/CYBERTEC-operator-tutorials.git cd CYBERTEC-operator-tutorials Step 2 - Install the Operator Two options are available for the installation:"},{"id":30,"href":"/CYBERTEC-pg-operator/release_notes/","title":"Release Notes","parent":"CPO (CYBERTEC-PG-Operator)","content":" 0.7.0 Features Monitoring-Sidecar integrated via CRD Start with Monitoring Password-Hash per default set to scram-sha-256 Changes API Change acid.zalan.do is replaced by cpo.opensource.cybertec.at - If you\u0026rsquo;re updating your Operator from previous Versions, please check this HowTo Migrate to new API Patroni-Compatibility has increased to Version 3.2.2 pgBackRest-Compatbility has increased to Version 2.50 Fixes PDB Bug fixed - Single-Node Clusters are not creating PDBs anymore which can break Kubernetes-Update Supported Versions PG: 12 - 16 Patroni: 3.2.2 pgBackRest: 2.50 Kubernetes: 1.21 - 1.28 Openshift: 4.8 - 4.13 0.6.1 Release with fixes\nFixes Backup-Pod now runs with \u0026ldquo;best-effort\u0026rdquo; resource definition Der Init-Container fr die Wiederherstellung verwendet nun die gleiche Ressource-Definition wie der Datenbank-Container, wenn es keine spezifische Definition im Cluster-Manifest gibt (spec.backup.pgbackrest.resources) Software-Versions PostgreSQL: 15.3 14.8, 13.11, 12.15 Patroni: 3.0.4 pgBackRest: 2.47 OS: Rocky-Linux 9.1 (4.18) 0.6.0 Release with some improvements and stabilisation measuresm\nFeatures Added Pod Topology Spread Constraints Added support for TDE based on the CYBERTEC PostgreSQL Enterprise Images (Licensed Container Suite) Software-Versions PostgreSQL: 15.3 14.8, 13.11, 12.15 Patroni: 3.0.4 pgBackRest: 2.47 OS: Rocky-Linux 9.1 (4.18) 0.5.0 Release with new Software-Updates and some internal Improvements\nFeatures Updated to Zalando Operator 1.9 Fixes internal Problems with Cronjobs updates for some API-Definitions Software-Versions PostgreSQL: 15.2 14.7, 13.10, 12.14 Patroni: 3.0.2 pgBackRest: 2.45 OS: Rocky-Linux 9.1 (4.18) 0.3.0 Release with some improvements and stabilisation measuresm\nFixes missing pgbackrest_restore configmap fixed Software-Versions PostgreSQL: 15.1 14.7, 13.9, 12.13, 11.18 and 10.23 Patroni: 3.0.1 pgBackRest: 2.44 OS: Rocky-Linux 9.1 (4.18) 0.1.0 Initial Release as a Fork of the Zalando-Operator\nFeatures Added Support for pgBackRest (PoC-State) Stanza-create and Initial-Backup are executed automatically Schedule automatic updates (Full/Incremental/Differential-Backup) Securely store backups on AWS S3 and S3-compatible storage Software-Versions PostgreSQL: 14.6, 13.9, 12.13, 11.18 and 10.23 Patroni: 2.4.1 pgBackRest: 2.42 OS: Rocky-Linux 9.0 (4.18) ","description":"0.7.0 Features Monitoring-Sidecar integrated via CRD Start with Monitoring Password-Hash per default set to scram-sha-256 Changes API Change acid.zalan.do is replaced by cpo.opensource.cybertec.at - If you\u0026rsquo;re updating your Operator from previous Versions, please check this HowTo Migrate to new API Patroni-Compatibility has increased to Version 3.2.2 pgBackRest-Compatbility has increased to Version 2.50 Fixes PDB Bug fixed - Single-Node Clusters are not creating PDBs anymore which can break Kubernetes-Update Supported Versions PG: 12 - 16 Patroni: 3."},{"id":31,"href":"/CYBERTEC-pg-operator/level-4/","title":"Roadmap","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":32,"href":"/CYBERTEC-pg-operator/level-3/","title":"Support","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""},{"id":33,"href":"/CYBERTEC-pg-operator/tags/","title":"Tags","parent":"CPO (CYBERTEC-PG-Operator)","content":"","description":""}]